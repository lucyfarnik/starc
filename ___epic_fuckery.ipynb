{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from distance import canon\n",
    "from _types import Reward\n",
    "from env import Env, RandomEnv\n",
    "from coverage_dist import get_state_dist, get_action_dist\n",
    "from reward import random_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this implementation go brrn't\n",
    "def slow_epic(reward: Reward, env: Env):\n",
    "  state_dist = get_state_dist(env)\n",
    "  action_dist = get_action_dist(env)\n",
    "\n",
    "  term1 = np.zeros((1, 1, env.n_s))\n",
    "  for s_prime in range(env.n_s):\n",
    "    for A, A_prob in enumerate(action_dist):\n",
    "      for S_prime, S_prime_prob in enumerate(state_dist):\n",
    "        prob = A_prob * S_prime_prob\n",
    "        term1[0, 0, s_prime] += prob * (\n",
    "          env.discount * reward[s_prime, A, S_prime]\n",
    "        ) \n",
    "\n",
    "  term2 = np.zeros((env.n_s, 1, 1))\n",
    "  for s in range(env.n_s):\n",
    "    for A, A_prob in enumerate(action_dist):\n",
    "      for S_prime, S_prime_prob in enumerate(state_dist):\n",
    "        prob = A_prob * S_prime_prob\n",
    "        term2[s, 0, 0] += prob * reward[s, A, S_prime]\n",
    "\n",
    "  term3 = 0\n",
    "  for S, S_prob in enumerate(state_dist):\n",
    "    for A, A_prob in enumerate(action_dist):\n",
    "      for S_prime, S_prime_prob in enumerate(state_dist):\n",
    "        prob = S_prob * A_prob * S_prime_prob\n",
    "        term3 += prob * env.discount * reward[S, A, S_prime]\n",
    "\n",
    "  return term1, term2, term3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.9, 0. ]]]),\n",
       " array([[[1.]],\n",
       " \n",
       "        [[0.]]]),\n",
       " 0.45)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# non-deterministic toy env\n",
    "n_s = 2\n",
    "n_a = 2\n",
    "discount = 0.9\n",
    "init_dist = np.array([0.5, 0.5])\n",
    "transition_dist = np.array([\n",
    "  [\n",
    "    [0.9, 0.1],\n",
    "    [0.3, 0.7],\n",
    "  ],\n",
    "  [\n",
    "    [0.2, 0.8],\n",
    "    [0.9, 0.1],\n",
    "  ],\n",
    "])\n",
    "reward = np.array([\n",
    "    [\n",
    "      [0, 0],\n",
    "      [-1, 5],\n",
    "    ],\n",
    "    [\n",
    "      [-2, 1],\n",
    "      [1, 0],\n",
    "    ],\n",
    "])\n",
    "\n",
    "env = Env(n_s, n_a, discount, init_dist, transition_dist)\n",
    "\n",
    "# if the implementation is correct and I'm not brain dead this should output\n",
    "# [0.9, 0], [1, 0], 0.45 (ignoring empty dims)\n",
    "slow_epic(reward, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.9, 0. ]]]),\n",
       " array([[[1.]],\n",
       " \n",
       "        [[0.]]]),\n",
       " 0.45)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def epic_brr(reward: Reward, env: Env):\n",
    "  D_s = get_state_dist(env)\n",
    "  D_a = get_action_dist(env)\n",
    "  S = D_s[:, None, None]\n",
    "  A = D_a[None, :, None]\n",
    "  S_prime = D_s[None, None, :]\n",
    "\n",
    "  potential = (reward * A * S_prime).sum(axis=(1, 2))\n",
    "\n",
    "  term1 = env.discount * potential[None, None, :]\n",
    "\n",
    "  term2 = potential[:, None, None]\n",
    "  \n",
    "  term3 = env.discount * (reward * S * A * S_prime).sum()\n",
    "\n",
    "  return term1, term2, term3\n",
    "\n",
    "epic_brr(reward, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "  e = RandomEnv(8, 2)\n",
    "  r = random_reward(e)\n",
    "  s1, s2, s3 = slow_epic(r, e)\n",
    "  b1, b2, b3 = epic_brr(r, e)\n",
    "  assert np.isclose(s1, b1).all()\n",
    "  assert np.isclose(s2, b2).all()\n",
    "  assert np.isclose(s3, b3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epic_brrnt(reward, env):\n",
    "  term1, term2, term3 = slow_epic(reward, env)\n",
    "  return reward + term1 - term2 - term3\n",
    "\n",
    "def epic_brrrr(reward, env):\n",
    "  term1, term2, term3 = epic_brr(reward, env)\n",
    "  return reward + term1 - term2 - term3\n",
    "\n",
    "for _ in range(10):\n",
    "  e = RandomEnv(8, 2)\n",
    "  r = random_reward(e)\n",
    "  assert np.isclose(epic_brrnt(r,e), epic_brrrr(r,e)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "178f5955e7ec2db83c487531b6f19a6ba078d2c9bcad0eaf79872b0fcb34bd80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
