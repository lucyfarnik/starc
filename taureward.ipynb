{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from env import Env, RandomEnv\n",
    "from reward import random_reward\n",
    "from _types import Reward\n",
    "from utils import timed\n",
    "from canon import epic_canon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import softmax\n",
    "\n",
    "class Env():\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_s: int,\n",
    "        n_a: int,\n",
    "        discount: float,\n",
    "        init_dist: np.ndarray,\n",
    "        transition_dist: np.ndarray,\n",
    "    ):\n",
    "        self.n_s = n_s\n",
    "        self.states = np.arange(n_s)\n",
    "        self.n_a = n_a\n",
    "        self.actions = np.arange(n_a)\n",
    "        self.discount = discount\n",
    "        self.init_dist = init_dist\n",
    "        self.transition_dist = transition_dist\n",
    "\n",
    "class RandomEnv(Env):\n",
    "    def __init__(self, n_s: int = 128, n_a: int = 16, discount: int = 0.9):\n",
    "        init_dist = np.ones(n_s) / n_s\n",
    "        thresh = 1 if n_s < 50 else (1.5 if n_s < 100 else 1.8)\n",
    "        transition_dist = np.random.randn(n_s, n_a, n_s)\n",
    "        transition_dist = np.where(transition_dist > thresh,\n",
    "                             transition_dist, np.zeros_like(transition_dist) - 20)\n",
    "        transition_dist = softmax(transition_dist)\n",
    "        super().__init__(n_s, n_a, discount, init_dist, transition_dist)\n",
    "\n",
    "    def modified_reward_matrix(self, original_reward_matrix: np.ndarray) -> np.ndarray:\n",
    "        n_s, n_a, _ = original_reward_matrix.shape\n",
    "        modified_reward_matrix = np.zeros((n_s, n_a, n_s))\n",
    "\n",
    "        for s in range(n_s):\n",
    "            for a in range(n_a):\n",
    "                expected_reward = np.sum(self.transition_dist[s, a] * original_reward_matrix[s, a])\n",
    "                modified_reward_matrix[s, a] = expected_reward\n",
    "\n",
    "        return modified_reward_matrix\n",
    "\n",
    "def random_reward(env: Env) -> np.ndarray:\n",
    "    r = np.random.randn(env.n_s, env.n_a, env.n_s)\n",
    "    if np.random.random() > 0.8:\n",
    "        thresh = 3 if env.n_s < 50 else (3.5 if env.n_s < 100 else 3.8)\n",
    "        r = np.where(r > thresh, r, np.zeros_like(r))\n",
    "    if np.random.random() > 0.3:\n",
    "        r *= 10 * np.random.random()\n",
    "    if np.random.random() > 0.7:\n",
    "        r += 10 * np.random.random()\n",
    "    if np.random.random() > 0.5:\n",
    "        potential = np.random.randn(env.n_s)\n",
    "        potential *= 10 * np.random.random()\n",
    "        potential += np.random.random()\n",
    "        r += env.discount * potential[None, None, :] - potential[:, None, None]\n",
    "    return r\n",
    "\n",
    "\n",
    "env = RandomEnv(n_s=128, n_a=16, discount=0.9)\n",
    "\n",
    "r1 = random_reward(env)\n",
    "\n",
    "r1_tau = env.modified_reward_matrix(r1)\n",
    "\n",
    "r2 = random_reward(env)\n",
    "\n",
    "r2_tau = env.modified_reward_matrix(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epic(r1: Reward, r2: Reward, env: Env) -> float:\n",
    "  r1_can = epic_canon(r1, env)\n",
    "  r2_can = epic_canon(r2, env)\n",
    "\n",
    "  r1_norm = r1_can / np.linalg.norm(r1_can.flatten(), 2)\n",
    "  r2_norm = r2_can / np.linalg.norm(r2_can.flatten(), 2)\n",
    "\n",
    "  return np.linalg.norm((r1_norm - r2_norm).flatten(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = epic(r1, r2, env)\n",
    "d2 = epic(r1_tau, r2_tau, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4147310858370963"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4618045597388316"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         d1        d2\n",
      "0  1.414067  1.415951\n",
      "1  1.417468  1.421807\n",
      "2  1.415292  1.459411\n",
      "3  1.413255  1.404478\n",
      "4  1.415144  1.427270\n",
      "5  1.413020  1.370182\n",
      "6  1.412565  1.374363\n",
      "7  1.416404  1.401488\n",
      "8  1.414125  1.429347\n",
      "9  1.414752  1.387329\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "results = pd.DataFrame(columns=[\"d1\", \"d2\"])\n",
    "\n",
    "for i in range(10):\n",
    "    env = RandomEnv(n_s=128, n_a=16, discount=0.9)\n",
    "    r1 = random_reward(env)\n",
    "    r1_tau = env.modified_reward_matrix(r1)\n",
    "\n",
    "    r2 = random_reward(env)\n",
    "    r2_tau = env.modified_reward_matrix(r2)\n",
    "\n",
    "    d1 = epic(r1, r2, env)\n",
    "    d2 = epic(r1_tau, r2_tau, env)\n",
    "\n",
    "    results.loc[i] = [d1, d2]\n",
    "\n",
    "print(results)\n",
    "\n",
    "#d1 is epic(r1, r2, env) and d2 is epic(r1_tau, r2_tau, env)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
  },
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
