{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Need to restructure everything, adding through file upload for now\n",
    "import numpy as np\n",
    "import torch\n",
    "from env import Env, RandomEnv\n",
    "from reward import random_reward\n",
    "from _types import Reward\n",
    "from utils import timed\n",
    "from canon import epic_canon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Illustrative rewards for gridworlds.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SPARSE_GOAL = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 1]])\n",
    "\n",
    "CENTER_GOAL = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n",
    "\n",
    "OBSTACLE_COURSE = np.array([[0, -1, -1], [0, 0, 0], [-1, -1, 4]])\n",
    "\n",
    "CLIFF_WALK = np.array([[0, -1, -1], [0, 0, 0], [-4, -4, 4]])\n",
    "\n",
    "MANHATTAN_FROM_GOAL = np.array([[4, 3, 2], [3, 2, 1], [2, 1, 0]])\n",
    "\n",
    "ZERO = np.zeros((3, 3))\n",
    "\n",
    "REWARDS = {\n",
    "    # Equivalent rewards\n",
    "    \"sparse_goal\": {\"state_reward\": SPARSE_GOAL, \"potential\": ZERO},\n",
    "    \"sparse_goal_shift\": {\"state_reward\": SPARSE_GOAL + 1, \"potential\": ZERO},\n",
    "    \"sparse_goal_scale\": {\"state_reward\": SPARSE_GOAL * 10, \"potential\": ZERO},\n",
    "    \"dense_goal\": {\"state_reward\": SPARSE_GOAL, \"potential\": -MANHATTAN_FROM_GOAL},\n",
    "    \"antidense_goal\": {\"state_reward\": SPARSE_GOAL, \"potential\": MANHATTAN_FROM_GOAL},\n",
    "    # Non-equivalent rewards\n",
    "    \"transformed_goal\": {\n",
    "        # Shifted, rescaled and reshaped sparse goal.\n",
    "        \"state_reward\": SPARSE_GOAL * 4 - 1,\n",
    "        \"potential\": -MANHATTAN_FROM_GOAL * 3,\n",
    "    },\n",
    "    \"center_goal\": {\n",
    "        # Goal is in center\n",
    "        \"state_reward\": CENTER_GOAL,\n",
    "        \"potential\": ZERO,\n",
    "    },\n",
    "    \"dirt_path\": {\n",
    "        # Some minor penalties to avoid to reach goal.\n",
    "        #\n",
    "        # Optimal policy for this is optimal in `SPARSE_GOAL`, but not equivalent.\n",
    "        # Think may come apart in some dynamics but not particularly intuitively.\n",
    "        \"state_reward\": OBSTACLE_COURSE,\n",
    "        \"potential\": ZERO,\n",
    "    },\n",
    "    \"cliff_walk\": {\n",
    "        # Avoid cliff to reach goal. Same set of optimal policies as `obstacle_course` in\n",
    "        # deterministic dynamics, but not equivalent.\n",
    "        #\n",
    "        # Optimal policy differs in sufficiently slippery gridworlds as want to stay on top line\n",
    "        # to avoid chance of falling off cliff.\n",
    "        \"state_reward\": CLIFF_WALK,\n",
    "        \"potential\": ZERO,\n",
    "    },\n",
    "    \"sparse_penalty\": {\n",
    "        # Negative of `sparse_goal`.\n",
    "        \"state_reward\": -SPARSE_GOAL,\n",
    "        \"potential\": ZERO,\n",
    "    },\n",
    "    \"evaluating_rewards/Zero-v0\": {\n",
    "        # All zero reward function\n",
    "        \"state_reward\": ZERO,\n",
    "        \"potential\": ZERO,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epic_canon(reward: Reward, env: Env) -> Reward:\n",
    "  D_s = get_state_dist(env)\n",
    "  D_a = get_action_dist(env)\n",
    "  if type(reward) is torch.Tensor:\n",
    "    D_s, D_a = torch.tensor(D_s), torch.tensor(D_a)\n",
    "  S = D_s[:, None, None]\n",
    "  A = D_a[None, :, None]\n",
    "  S_prime = D_s[None, None, :]\n",
    "\n",
    "  potential = (reward * A * S_prime).sum(axis=(1, 2))\n",
    "\n",
    "  term1 = env.discount * potential[None, None, :]\n",
    "  term2 = potential[:, None, None]\n",
    "  term3 = env.discount * (reward * S * A * S_prime).sum()\n",
    "\n",
    "  return reward + term1 - term2 - term3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epic(r1: Reward, r2: Reward, env: Env) -> float:\n",
    "  r1_can = epic_canon(r1, env)\n",
    "  r2_can = epic_canon(r2, env)\n",
    "\n",
    "  r1_norm = r1_can / np.linalg.norm(r1_can.flatten(), 2)\n",
    "  r2_norm = r2_can / np.linalg.norm(r2_can.flatten(), 2)\n",
    "\n",
    "  return np.linalg.norm((r1_norm - r2_norm).flatten(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cliff Walk Reward Matrix:\n",
      "[[ 0 -1 -1]\n",
      " [ 0  0  0]\n",
      " [-4 -4  4]]\n",
      "Dense Walk Reward Matrix:\n",
      "[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "cliff_var = REWARDS[\"cliff_walk\"][\"state_reward\"]\n",
    "dense_var = REWARDS[\"dense_goal\"][\"state_reward\"]\n",
    "\n",
    "cliff_var_f = cliff_var.flatten()\n",
    "dense_var_f = dense_var.flatten()\n",
    "\n",
    "zero_s = np.zeros((9,5,9))\n",
    "zero_d = np.zeros((9,5,9))\n",
    "\n",
    "zero_s += cliff_var_f[:,None,None]\n",
    "zero_d += dense_var_f[:,None,None]\n",
    "\n",
    "print(\"Cliff Walk Reward Matrix:\")\n",
    "print(cliff_var)\n",
    "\n",
    "print(\"Dense Walk Reward Matrix:\")\n",
    "print(dense_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import softmax\n",
    "\n",
    "class Env():\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_s: int,\n",
    "        n_a: int,\n",
    "        discount: float,\n",
    "        init_dist: np.ndarray,\n",
    "        transition_dist: np.ndarray,\n",
    "    ):\n",
    "        self.n_s = n_s\n",
    "        self.states = np.arange(n_s)\n",
    "        self.n_a = n_a\n",
    "        self.actions = np.arange(n_a)\n",
    "        self.discount = discount\n",
    "        self.init_dist = init_dist\n",
    "        self.transition_dist = transition_dist\n",
    "class RandomEnv(Env):\n",
    "    def __init__(self, n_s: int = 128, n_a: int = 16, discount: int = 0.9):\n",
    "        init_dist = np.ones(n_s) / n_s\n",
    "        thresh = 1 if n_s < 50 else (1.5 if n_s < 100 else 1.8)\n",
    "        transition_dist = np.random.randn(n_s, n_a, n_s)\n",
    "        transition_dist = np.where(transition_dist > thresh,\n",
    "                             transition_dist, np.zeros_like(transition_dist) - 20)\n",
    "                             \n",
    "        transition_dist = softmax(transition_dist)\n",
    "        super().__init__(n_s, n_a, discount, init_dist, transition_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cliff_var = REWARDS['cliff_walk']['state_reward']\n",
    "dense_var = REWARDS['dense_goal']['state_reward']\n",
    "\n",
    "cliff_env = RandomEnv(n_s=9, n_a=5, discount=0.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3675592134610681"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epic(zero_s, zero_d, cliff_env)/2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
  },
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
