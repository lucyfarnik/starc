{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DO NOT LOOK AT THIS CODE\n",
    "#It repeats everything to run this experiment because I was facing import issues in the main repo\n",
    "#Essentially useless, ignore but keep for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from the EPIC paper section 5\n",
    "import numpy as np\n",
    "from functools import wraps\n",
    "import time\n",
    "import numpy as np\n",
    "Reward = np.ndarray\n",
    "import numpy as np\n",
    "\n",
    "def softmax(arr: np.ndarray) -> np.ndarray:\n",
    "  exp = np.exp(arr)\n",
    "  norm = np.sum(exp, axis=-1)\n",
    "  norm = np.reshape(norm, (*arr.shape[0:-1], 1))\n",
    "  norm = np.repeat(norm, arr.shape[-1], axis=-1)\n",
    "  return exp / norm\n",
    "  \n",
    "def timed(f):\n",
    "  @wraps(f)\n",
    "  def wrapped(*args, **kwargs):\n",
    "    st = time.perf_counter()\n",
    "    out = f(*args, **kwargs)\n",
    "    et = time.perf_counter()\n",
    "    print(f'{f.__name__} took {et-st:.4f}s')\n",
    "    return out\n",
    "  return wrapped\n",
    "\n",
    "class Env():\n",
    "  def __init__(\n",
    "    self,\n",
    "    n_s: int,\n",
    "    n_a: int,\n",
    "    discount: float,\n",
    "    init_dist: np.ndarray,\n",
    "    transition_dist: np.ndarray,\n",
    "  ):\n",
    "    self.n_s = n_s\n",
    "    self.states = np.arange(n_s)\n",
    "    self.n_a = n_a\n",
    "    self.actions = np.arange(n_a)\n",
    "    self.discount = discount\n",
    "    self.init_dist = init_dist\n",
    "    self.transition_dist = transition_dist\n",
    "\n",
    "class RandomEnv(Env):\n",
    "  def __init__(self, n_s: int = 128, n_a: int = 16, discount: int = 0.9,\n",
    "               episodic: bool = False):\n",
    "    \"\"\"\n",
    "      Create a random environment.\n",
    "      n_s: number of states\n",
    "      n_a: number of actions\n",
    "      discount: discount factor\n",
    "      episodic: whether the environment is episodic - in this case episodic\n",
    "        means that there is a terminal (self-looping) state\n",
    "    \"\"\"\n",
    "    # uniform init dist\n",
    "    init_dist = np.ones(n_s) / n_s\n",
    "\n",
    "    # sample iid Gaussians, then only keep the highest values, then softmax -> sparse\n",
    "    # TODO we currently pick the largest values as being 1.8sigma above mean\n",
    "    # this means that if we have larger n_s we'll be picking more values\n",
    "    # at this stage which will make transitions more uniform again\n",
    "    thresh = 1 if n_s < 50 else (1.5 if n_s < 100 else 1.8) #! bigly goodn't hacky shit, kinda works for 32, 64, and 128\n",
    "    transition_dist = np.random.randn(n_s, n_a, n_s)\n",
    "    transition_dist = np.where(transition_dist > thresh,\n",
    "                         transition_dist, np.zeros_like(transition_dist)-20)\n",
    "    transition_dist = softmax(transition_dist)\n",
    "\n",
    "    self.episodic = episodic\n",
    "    if episodic:\n",
    "      self.terminal_state = np.random.randint(0, n_s)\n",
    "      for a in range(n_a):\n",
    "        for s_prime in range(n_s):\n",
    "          prob = 1.0 if s_prime == self.terminal_state else 0.0\n",
    "          transition_dist[self.terminal_state, a, s_prime] = prob\n",
    "\n",
    "    super().__init__(n_s, n_a, discount, init_dist, transition_dist)\n",
    "\n",
    "\"\"\"Illustrative rewards for gridworlds.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "def epic_gridworlds():\n",
    "    SPARSE_GOAL = np.array([[0, 0, 0], [0, 0, 0], [0, 0, 1]])\n",
    "\n",
    "    CENTER_GOAL = np.array([[0, 0, 0], [0, 1, 0], [0, 0, 0]])\n",
    "\n",
    "    OBSTACLE_COURSE = np.array([[0, -1, -1], [0, 0, 0], [-1, -1, 4]])\n",
    "\n",
    "    CLIFF_WALK = np.array([[0, -1, -1], [0, 0, 0], [-4, -4, 4]])\n",
    "\n",
    "    MANHATTAN_FROM_GOAL = np.array([[4, 3, 2], [3, 2, 1], [2, 1, 0]])\n",
    "\n",
    "    ZERO = np.zeros((3, 3))\n",
    "\n",
    "    REWARDS = {\n",
    "        # Equivalent rewards\n",
    "        \"sparse_goal\": {\"state_reward\": SPARSE_GOAL, \"potential\": ZERO},\n",
    "        \"sparse_goal_shift\": {\"state_reward\": SPARSE_GOAL + 1, \"potential\": ZERO},\n",
    "        \"sparse_goal_scale\": {\"state_reward\": SPARSE_GOAL * 10, \"potential\": ZERO},\n",
    "        \"dense_goal\": {\"state_reward\": SPARSE_GOAL, \"potential\": -MANHATTAN_FROM_GOAL},\n",
    "        \"antidense_goal\": {\"state_reward\": SPARSE_GOAL, \"potential\": MANHATTAN_FROM_GOAL},\n",
    "        # Non-equivalent rewards\n",
    "        \"transformed_goal\": {\n",
    "            # Shifted, rescaled and reshaped sparse goal.\n",
    "            \"state_reward\": SPARSE_GOAL * 4 - 1,\n",
    "            \"potential\": -MANHATTAN_FROM_GOAL * 3,\n",
    "        },\n",
    "        \"center_goal\": {\n",
    "            # Goal is in center\n",
    "            \"state_reward\": CENTER_GOAL,\n",
    "            \"potential\": ZERO,\n",
    "        },\n",
    "        \"dirt_path\": {\n",
    "            # Some minor penalties to avoid to reach goal.\n",
    "            #\n",
    "            # Optimal policy for this is optimal in `SPARSE_GOAL`, but not equivalent.\n",
    "            # Think may come apart in some dynamics but not particularly intuitively.\n",
    "            \"state_reward\": OBSTACLE_COURSE,\n",
    "            \"potential\": ZERO,\n",
    "        },\n",
    "        \"cliff_walk\": {\n",
    "            # Avoid cliff to reach goal. Same set of optimal policies as `obstacle_course` in\n",
    "            # deterministic dynamics, but not equivalent.\n",
    "            #\n",
    "            # Optimal policy differs in sufficiently slippery gridworlds as want to stay on top line\n",
    "            # to avoid chance of falling off cliff.\n",
    "            \"state_reward\": CLIFF_WALK,\n",
    "            \"potential\": ZERO,\n",
    "        },\n",
    "        \"sparse_penalty\": {\n",
    "            # Negative of `sparse_goal`.\n",
    "            \"state_reward\": -SPARSE_GOAL,\n",
    "            \"potential\": ZERO,\n",
    "        },\n",
    "        \"evaluating_rewards/Zero-v0\": {\n",
    "            # All zero reward function\n",
    "            \"state_reward\": ZERO,\n",
    "            \"potential\": ZERO,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    sparse_var = REWARDS[\"sparse_goal\"][\"state_reward\"]\n",
    "    dense_var = REWARDS[\"dense_goal\"][\"state_reward\"]\n",
    "    cliff_var = REWARDS[\"cliff_walk\"][\"state_reward\"]\n",
    "    path_var = REWARDS[\"dirt_path\"][\"state_reward\"]\n",
    "\n",
    "    cliff_var_f = cliff_var.flatten()\n",
    "    dense_var_f = dense_var.flatten()\n",
    "    sparse_var_f = sparse_var.flatten()\n",
    "    path_var_f = path_var.flatten()\n",
    "\n",
    "    sparse_reward = np.zeros((9,5,9)) + sparse_var_f[:,None,None]\n",
    "    dense_reward = np.zeros((9,5,9)) + dense_var_f[:,None,None]\n",
    "    path_reward = np.zeros((9,5,9)) + path_var_f[:,None,None]\n",
    "    cliff_reward = np.zeros((9,5,9)) + cliff_var_f[:,None,None]\n",
    "\n",
    "    r = {'sparse': sparse_reward,\n",
    "        'dense': dense_reward,\n",
    "        'path': path_reward,\n",
    "        'cliff': cliff_reward}\n",
    "\n",
    "    env = RandomEnv(n_s=9, n_a=5, discount=0.99) \n",
    "    return env, r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "import torch\n",
    "from einops import rearrange\n",
    "Policy = np.ndarray \n",
    "def get_state_dist(env: Env): # prob of transitioning into state\n",
    "  return np.ones(env.n_s) / env.n_s\n",
    "  # logits = env.transition_dist.sum(axis=(0, 1))\n",
    "  # sum = logits.sum()\n",
    "  # return logits / sum\n",
    "\n",
    "def get_action_dist(env: Env): # right now this is just uniform\n",
    "  return np.ones(env.n_a) / env.n_a\n",
    "\n",
    "def epic_canon(reward: Reward, env: Env) -> Reward:\n",
    "  D_s = get_state_dist(env)\n",
    "  D_a = get_action_dist(env)\n",
    "  if type(reward) is torch.Tensor:\n",
    "    D_s, D_a = torch.tensor(D_s), torch.tensor(D_a)\n",
    "  S = D_s[:, None, None]\n",
    "  A = D_a[None, :, None]\n",
    "  S_prime = D_s[None, None, :]\n",
    "\n",
    "  potential = (reward * A * S_prime).sum(axis=(1, 2))\n",
    "\n",
    "  term1 = env.discount * potential[None, None, :]\n",
    "  term2 = potential[:, None, None]\n",
    "  term3 = env.discount * (reward * S * A * S_prime).sum()\n",
    "\n",
    "  return reward + term1 - term2 - term3\n",
    "\n",
    "def dard_canon(reward: Reward, env: Env) -> Reward:\n",
    "  A = get_action_dist(env)\n",
    "\n",
    "  potential = (env.transition_dist * reward).sum(axis=2)\n",
    "  potential = (potential * A[None, :]).sum(axis=1)\n",
    "\n",
    "  term1 = env.discount * potential[None, None, :]\n",
    "  term2 = potential[:, None, None]\n",
    "  \n",
    "  joint_probs = ( # [s, s', S', A, S'']; p(S', S'' | s, s', A=A)\n",
    "    A[None, None, None, :, None] * \n",
    "    rearrange(env.transition_dist, 's A Sp -> s 1 Sp A 1') *\n",
    "    rearrange(env.transition_dist, 'sp A Sd -> 1 sp 1 A Sd')\n",
    "  )\n",
    "  r_given_probs = reward[None, None, ...] * joint_probs\n",
    "  term3 = env.discount * r_given_probs.sum(axis=(2,3,4))[:,None,:]\n",
    "  \n",
    "  return reward + term1 - term2 - term3\n",
    "\n",
    "#! Does not converge for norm_ord 1 or inf\n",
    "# @timed\n",
    "def minimal_canon(\n",
    "    reward: Reward, env: Env, norm_ord: Union[int, float], max_iters=100000,\n",
    ") -> Reward:\n",
    "  r = torch.tensor(reward)\n",
    "  # potential = torch.tensor(reward.mean(axis=(1, 2)), requires_grad=True)\n",
    "  potential = torch.zeros(env.n_s, requires_grad=True)\n",
    "  frozen_potential = torch.clone(potential) \n",
    "  \n",
    "  optimizer = torch.optim.Adam([potential], lr=1e-2)\n",
    "  for i in range(max_iters):\n",
    "    optimizer.zero_grad()\n",
    "    r_prime = r + env.discount * potential[None, None, :] - potential[:, None, None]\n",
    "    loss = torch.norm(r_prime, norm_ord)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # convergence = small gradient or potential hasn't changed in a while\n",
    "    if torch.norm(potential.grad, 2) < 1e-4: break\n",
    "    if i%10000 == 0 and i != 0:\n",
    "      if torch.isclose(potential, frozen_potential, rtol=1e-3, atol=1e-3).all():\n",
    "        # print(i)\n",
    "        break\n",
    "      else: frozen_potential = torch.clone(potential)\n",
    "    if i==max_iters-1: print(\"Didn't converge\")\n",
    "  return r_prime.detach().numpy()\n",
    "\n",
    "canon_funcs = {\n",
    "  'None': lambda r, _: r,\n",
    "  'EPIC': epic_canon,\n",
    "  'DARD': dard_canon,\n",
    "  'Minimal': minimal_canon,\n",
    "}\n",
    "\n",
    "# computes either the norm, or returns 1 if ord==0\n",
    "# which makes it useful in defining canon_and_norm (where norm==0 means don't normalize)\n",
    "def norm_wrapper(reward: Reward, ord: Union[int, float]) -> float:\n",
    "  if ord == 0: return 1\n",
    "  return np.linalg.norm(reward.flatten(), ord)\n",
    "\n",
    "norm_opts = [1, 2, float('inf'), 0]\n",
    "# returns a dictionary of all the possible canonicalizations and normalizations\n",
    "def canon_and_norm(reward: Reward, env: Env) -> dict[str, Reward]:\n",
    "  can = {c_name: canon_funcs[c_name](reward, env)\n",
    "         for c_name in ['None', 'EPIC', 'DARD']}\n",
    "  norm = {f'{c_name}-{n_ord}': val / norm_wrapper(val, n_ord)\n",
    "            for n_ord in norm_opts\n",
    "            for c_name, val in can.items()}\n",
    "  # add in minimal canon (which depends on the norm order so it needs different code)\n",
    "  for n_ord in norm_opts:\n",
    "    if n_ord != 2: continue #! REMOVE ME\n",
    "    if n_ord == 0: continue\n",
    "    min_can = canon_funcs['Minimal'](reward, env, n_ord)\n",
    "    norm[f'Minimal-{n_ord}'] = min_can / norm_wrapper(min_can, n_ord)\n",
    "  return norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def optimize(env: Env, reward: Reward, convergence_thresh=1e-5) -> Policy:\n",
    "  state_vals = np.zeros(env.n_s)\n",
    "\n",
    "  for _ in range(10000):\n",
    "    cond_p = env.transition_dist * (reward + env.discount * state_vals[None, None, :])\n",
    "    new_vals = cond_p.sum(axis=2).max(axis=1)\n",
    "    diff = state_vals - new_vals\n",
    "    state_vals = new_vals\n",
    "    if np.linalg.norm(diff, 2) < convergence_thresh: break\n",
    "  \n",
    "  return cond_p.sum(axis=2).argmax(axis=1)\n",
    "\n",
    "# Monte Carlo estimation\n",
    "def policy_returns(\n",
    "  rewards: list[Reward],\n",
    "  policy: Policy,\n",
    "  env: Env,\n",
    "  discount_thresh=1e-5,\n",
    ") -> list[float]:\n",
    "  # beyond this point, the discounts get so heavy that it's not worth computing\n",
    "  steps_per_episode = round(np.log(discount_thresh) / np.log(env.discount))\n",
    "\n",
    "  num_rs = len(rewards)\n",
    "\n",
    "  # 2D array, first dim is different reward funcs, second dim is samples\n",
    "  return_vals = [[] for _ in range(num_rs)]\n",
    "\n",
    "  for _ in range(env.n_a): # do multiple repetitions to ensure low variance\n",
    "    for episode_i in range(env.n_s):\n",
    "      # init state - we want to have one episode for each possible starting state\n",
    "      s = episode_i\n",
    "      episode_rewards = [[] for _ in range(num_rs)] # same dims as return_vals\n",
    "\n",
    "      for _ in range(steps_per_episode):\n",
    "        # # sample action from policy\n",
    "        # a = np.random.choice(env.actions, p=policy[s])\n",
    "        a = policy[s]\n",
    "\n",
    "        # next state\n",
    "        s_next = np.random.choice(env.states, p=env.transition_dist[s, a])\n",
    "        for i, r in enumerate(rewards):\n",
    "          episode_rewards[i].append(r[s, a, s_next])\n",
    "        s = s_next\n",
    "      \n",
    "      # at the end we compute the discounted return return\n",
    "      for r_i, r_values in enumerate(episode_rewards): # for each return func\n",
    "        return_val = 0 # accumulator for the return\n",
    "        for i, r in enumerate(r_values):\n",
    "          if i == 0: gamma_i = 1.0\n",
    "          else: gamma_i *= env.discount\n",
    "          return_val += gamma_i * r\n",
    "        return_vals[r_i].append(return_val)\n",
    "\n",
    "  return [sum(rs) / len(rs) for rs in return_vals]\n",
    "  \n",
    "# wrapper for the function above - takes just one reward function\n",
    "def policy_return(reward: Reward, *args, **kwargs) -> Reward:\n",
    "  return policy_returns([reward], *args, **kwargs)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse-dense\n",
      "sparse-path\n",
      "sparse-cliff\n",
      "dense-path\n",
      "dense-cliff\n",
      "path-cliff\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import json\n",
    "def handpicked_experiment(results_path: str):\n",
    "\n",
    "  dist_opts = [1, 2, float('inf')]\n",
    "\n",
    "  np.random.seed(42)\n",
    "\n",
    "  env, rewards = epic_gridworlds()\n",
    "\n",
    "  # experiment starts here\n",
    "  results = {}\n",
    "  for (r1_name, r1), (r2_name, r2) in itertools.combinations(rewards.items(), 2):\n",
    "    r_names = f'{r1_name}-{r2_name}'\n",
    "    print(r_names)\n",
    "    results[r_names] = {}\n",
    "\n",
    "    # canonicalizations and normalizations of R1 and R2\n",
    "    can1 = canon_and_norm(r1, env)\n",
    "    can2 = canon_and_norm(r2, env)\n",
    "\n",
    "    # compute the distances for all combinations of canon, norm, and dist\n",
    "    for cn_name, r1_val in can1.items():\n",
    "      r2_val = can2[cn_name]\n",
    "      for d_ord in dist_opts:\n",
    "        results[r_names][f'{cn_name}-{d_ord}'] = np.linalg.norm(\n",
    "          (r1_val - r2_val).flatten(), d_ord\n",
    "        )\n",
    "\n",
    "    # calculate the best and worst policies under R1 and R2\n",
    "    pi_1 = optimize(env, r1) # best policy under R1\n",
    "    pi_x = optimize(env, -r1) # worst policy under R1\n",
    "    pi_2 = optimize(env, r2) # best policy under R2\n",
    "    pi_y = optimize(env, -r2) # worst policy under R2\n",
    "\n",
    "    # compute return values\n",
    "    J_1_pi_1, J_2_pi_1 = policy_returns([r1, r2], pi_1, env)\n",
    "    J_1_pi_2, J_2_pi_2 = policy_returns([r1, r2], pi_2, env)\n",
    "    J_1_pi_x = policy_return(r1, pi_x, env)\n",
    "    J_2_pi_y = policy_return(r2, pi_y, env)\n",
    "    # compute regrets (normalizing over max-min)\n",
    "    results[r_names]['regret1'] = (J_1_pi_1 - J_1_pi_2) / (J_1_pi_1 - J_1_pi_x)\n",
    "    results[r_names]['regret2'] = (J_2_pi_2 - J_2_pi_1) / (J_2_pi_2 - J_2_pi_y)\n",
    "\n",
    "  # save as JSON\n",
    "  with open(results_path, 'w') as f:\n",
    "    json.dump(results, f)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  results_path = 'results/handpicked_epic_gridworlds_new.json'\n",
    "  handpicked_experiment(results_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPIC-1-1</th>\n",
       "      <th>EPIC-1-2</th>\n",
       "      <th>EPIC-1-inf</th>\n",
       "      <th>DARD-1-1</th>\n",
       "      <th>DARD-1-2</th>\n",
       "      <th>DARD-1-inf</th>\n",
       "      <th>EPIC-2-1</th>\n",
       "      <th>EPIC-2-2</th>\n",
       "      <th>EPIC-2-inf</th>\n",
       "      <th>DARD-2-1</th>\n",
       "      <th>...</th>\n",
       "      <th>EPIC-inf-2</th>\n",
       "      <th>EPIC-inf-inf</th>\n",
       "      <th>DARD-inf-1</th>\n",
       "      <th>DARD-inf-2</th>\n",
       "      <th>DARD-inf-inf</th>\n",
       "      <th>Minimal-2-1</th>\n",
       "      <th>Minimal-2-2</th>\n",
       "      <th>Minimal-2-inf</th>\n",
       "      <th>regret1</th>\n",
       "      <th>regret2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sparse-dense</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>-0.002591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sparse-path</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.026352</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.542596</td>\n",
       "      <td>0.030021</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>6.324555</td>\n",
       "      <td>0.320364</td>\n",
       "      <td>0.017568</td>\n",
       "      <td>6.264817</td>\n",
       "      <td>...</td>\n",
       "      <td>2.371708</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>41.967300</td>\n",
       "      <td>2.320334</td>\n",
       "      <td>0.217393</td>\n",
       "      <td>4.967565</td>\n",
       "      <td>0.320364</td>\n",
       "      <td>0.024836</td>\n",
       "      <td>-0.003894</td>\n",
       "      <td>0.041638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sparse-cliff</th>\n",
       "      <td>1.022727</td>\n",
       "      <td>0.055204</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>1.014147</td>\n",
       "      <td>0.062180</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>13.443116</td>\n",
       "      <td>0.735118</td>\n",
       "      <td>0.055696</td>\n",
       "      <td>12.427736</td>\n",
       "      <td>...</td>\n",
       "      <td>6.665285</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>96.751558</td>\n",
       "      <td>6.777368</td>\n",
       "      <td>0.812197</td>\n",
       "      <td>12.679754</td>\n",
       "      <td>0.735122</td>\n",
       "      <td>0.078891</td>\n",
       "      <td>0.060386</td>\n",
       "      <td>0.109954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense-path</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.026352</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.542596</td>\n",
       "      <td>0.030021</td>\n",
       "      <td>0.003640</td>\n",
       "      <td>6.324555</td>\n",
       "      <td>0.320364</td>\n",
       "      <td>0.017568</td>\n",
       "      <td>6.264817</td>\n",
       "      <td>...</td>\n",
       "      <td>2.371708</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>41.967300</td>\n",
       "      <td>2.320334</td>\n",
       "      <td>0.217393</td>\n",
       "      <td>4.967565</td>\n",
       "      <td>0.320364</td>\n",
       "      <td>0.024836</td>\n",
       "      <td>0.018071</td>\n",
       "      <td>0.029211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense-cliff</th>\n",
       "      <td>1.022727</td>\n",
       "      <td>0.055204</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>1.014147</td>\n",
       "      <td>0.062180</td>\n",
       "      <td>0.007143</td>\n",
       "      <td>13.443116</td>\n",
       "      <td>0.735118</td>\n",
       "      <td>0.055696</td>\n",
       "      <td>12.427736</td>\n",
       "      <td>...</td>\n",
       "      <td>6.665285</td>\n",
       "      <td>0.589286</td>\n",
       "      <td>96.751558</td>\n",
       "      <td>6.777368</td>\n",
       "      <td>0.812197</td>\n",
       "      <td>12.679754</td>\n",
       "      <td>0.735122</td>\n",
       "      <td>0.078891</td>\n",
       "      <td>0.054593</td>\n",
       "      <td>0.117298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>path-cliff</th>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.042989</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.653546</td>\n",
       "      <td>0.040422</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>9.956283</td>\n",
       "      <td>0.534426</td>\n",
       "      <td>0.039931</td>\n",
       "      <td>8.974813</td>\n",
       "      <td>...</td>\n",
       "      <td>5.093513</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>70.730825</td>\n",
       "      <td>5.246258</td>\n",
       "      <td>0.601111</td>\n",
       "      <td>9.391973</td>\n",
       "      <td>0.534430</td>\n",
       "      <td>0.056587</td>\n",
       "      <td>0.054938</td>\n",
       "      <td>0.058804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              EPIC-1-1  EPIC-1-2  EPIC-1-inf  DARD-1-1  DARD-1-2  DARD-1-inf  \\\n",
       "sparse-dense  0.000000  0.000000    0.000000  0.000000  0.000000    0.000000   \n",
       "sparse-path   0.500000  0.026352    0.001389  0.542596  0.030021    0.003640   \n",
       "sparse-cliff  1.022727  0.055204    0.004040  1.014147  0.062180    0.007143   \n",
       "dense-path    0.500000  0.026352    0.001389  0.542596  0.030021    0.003640   \n",
       "dense-cliff   1.022727  0.055204    0.004040  1.014147  0.062180    0.007143   \n",
       "path-cliff    0.772727  0.042989    0.004040  0.653546  0.040422    0.003986   \n",
       "\n",
       "               EPIC-2-1  EPIC-2-2  EPIC-2-inf   DARD-2-1  ...  EPIC-inf-2  \\\n",
       "sparse-dense   0.000000  0.000000    0.000000   0.000000  ...    0.000000   \n",
       "sparse-path    6.324555  0.320364    0.017568   6.264817  ...    2.371708   \n",
       "sparse-cliff  13.443116  0.735118    0.055696  12.427736  ...    6.665285   \n",
       "dense-path     6.324555  0.320364    0.017568   6.264817  ...    2.371708   \n",
       "dense-cliff   13.443116  0.735118    0.055696  12.427736  ...    6.665285   \n",
       "path-cliff     9.956283  0.534426    0.039931   8.974813  ...    5.093513   \n",
       "\n",
       "              EPIC-inf-inf  DARD-inf-1  DARD-inf-2  DARD-inf-inf  Minimal-2-1  \\\n",
       "sparse-dense      0.000000    0.000000    0.000000      0.000000     0.000000   \n",
       "sparse-path       0.125000   41.967300    2.320334      0.217393     4.967565   \n",
       "sparse-cliff      0.589286   96.751558    6.777368      0.812197    12.679754   \n",
       "dense-path        0.125000   41.967300    2.320334      0.217393     4.967565   \n",
       "dense-cliff       0.589286   96.751558    6.777368      0.812197    12.679754   \n",
       "path-cliff        0.464286   70.730825    5.246258      0.601111     9.391973   \n",
       "\n",
       "              Minimal-2-2  Minimal-2-inf   regret1   regret2  \n",
       "sparse-dense     0.000000       0.000000  0.002584 -0.002591  \n",
       "sparse-path      0.320364       0.024836 -0.003894  0.041638  \n",
       "sparse-cliff     0.735122       0.078891  0.060386  0.109954  \n",
       "dense-path       0.320364       0.024836  0.018071  0.029211  \n",
       "dense-cliff      0.735122       0.078891  0.054593  0.117298  \n",
       "path-cliff       0.534430       0.056587  0.054938  0.058804  \n",
       "\n",
       "[6 rows x 23 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is a temporary notebook for handpicked_new and epic_girdworlds_new.py until imports are fixed in the repo\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "slippery = False\n",
    "\n",
    "results = None\n",
    "path = f\"results/handpicked_epic_gridworlds_new.json\"\n",
    "with open(path, 'r') as f:\n",
    "  results = json.load(f)\n",
    "df = pd.DataFrame(results).T\n",
    "cols_to_drop = [col for col in df.columns if '0' in col or 'None' in col]\n",
    "df = df.drop(cols_to_drop, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPIC-2-2</th>\n",
       "      <th>DARD-2-2</th>\n",
       "      <th>Minimal-2-2</th>\n",
       "      <th>regret1</th>\n",
       "      <th>regret2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sparse-dense</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002584</td>\n",
       "      <td>-0.002591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sparse-path</th>\n",
       "      <td>0.320364</td>\n",
       "      <td>0.337912</td>\n",
       "      <td>0.320364</td>\n",
       "      <td>-0.003894</td>\n",
       "      <td>0.041638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sparse-cliff</th>\n",
       "      <td>0.735118</td>\n",
       "      <td>0.766884</td>\n",
       "      <td>0.735122</td>\n",
       "      <td>0.060386</td>\n",
       "      <td>0.109954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense-path</th>\n",
       "      <td>0.320364</td>\n",
       "      <td>0.337912</td>\n",
       "      <td>0.320364</td>\n",
       "      <td>0.018071</td>\n",
       "      <td>0.029211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dense-cliff</th>\n",
       "      <td>0.735118</td>\n",
       "      <td>0.766884</td>\n",
       "      <td>0.735122</td>\n",
       "      <td>0.054593</td>\n",
       "      <td>0.117298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>path-cliff</th>\n",
       "      <td>0.534426</td>\n",
       "      <td>0.558413</td>\n",
       "      <td>0.534430</td>\n",
       "      <td>0.054938</td>\n",
       "      <td>0.058804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              EPIC-2-2  DARD-2-2  Minimal-2-2   regret1   regret2\n",
       "sparse-dense  0.000000  0.000000     0.000000  0.002584 -0.002591\n",
       "sparse-path   0.320364  0.337912     0.320364 -0.003894  0.041638\n",
       "sparse-cliff  0.735118  0.766884     0.735122  0.060386  0.109954\n",
       "dense-path    0.320364  0.337912     0.320364  0.018071  0.029211\n",
       "dense-cliff   0.735118  0.766884     0.735122  0.054593  0.117298\n",
       "path-cliff    0.534426  0.558413     0.534430  0.054938  0.058804"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [col for col in df.columns if '2-2' in col or 'regret' in col]\n",
    "df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
  },
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
