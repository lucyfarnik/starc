{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from env import Env, RandomEnv\n",
    "from reward import random_reward\n",
    "from _types import Reward, Policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(env: Env, reward: Reward) -> Policy:\n",
    "  state_vals = np.zeros(env.n_s)\n",
    "\n",
    "  for i in range(10000):\n",
    "    cond_p = env.transition_dist * (reward + env.discount * state_vals[None, None, :])\n",
    "    new_vals = cond_p.sum(axis=2).max(axis=1)\n",
    "    diff = state_vals - new_vals\n",
    "    state_vals = new_vals\n",
    "    if np.linalg.norm(diff, 2) < 1e-5:\n",
    "      break\n",
    "  \n",
    "  return cond_p.sum(axis=2).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  7,  0, 15,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,\n",
       "        0,  0,  0,  2,  0,  0,  0,  0, 14,  0,  0,  0,  0,  0,  1,  0,  0,\n",
       "        8,  5,  0,  0,  0, 15,  0, 10,  7,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  7,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0, 11,  0, 12,  0,  0,  0,  0,  0,  0,  0, 15,  0,  0,  0,\n",
       "        0,  0,  0,  4,  0,  0,  0, 14,  0,  0,  0,  0,  0,  0,  0,  0,  8,\n",
       "        5,  0,  0,  0,  0,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0, 12,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = RandomEnv()\n",
    "r = random_reward(e)\n",
    "optimize(e, r)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea: have one episode per state, start in each state exactly once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_returns(\n",
    "  rewards: list[Reward],\n",
    "  policy: Policy,\n",
    "  env: Env,\n",
    "  discount_thresh=1e-5,\n",
    ") -> list[float]:\n",
    "  # beyond this point, the discounts get so heavy that it's not worth computing\n",
    "  steps_per_episode = round(np.log(discount_thresh) / np.log(env.discount))\n",
    "\n",
    "  num_rs = len(rewards)\n",
    "\n",
    "  # 2D array, first dim is different reward funcs, second dim is samples\n",
    "  return_vals = [[] for _ in range(num_rs)]\n",
    "\n",
    "  for episode_i in range(env.n_s):\n",
    "    # init state - we want to have one episode for each possible starting state\n",
    "    s = episode_i\n",
    "    episode_rewards = [[] for _ in range(num_rs)] # same dims as return_vals\n",
    "\n",
    "    for _ in range(steps_per_episode):\n",
    "      # # sample action from policy\n",
    "      # a = np.random.choice(env.actions, p=policy[s])\n",
    "      a = policy[s]\n",
    "\n",
    "      # next state\n",
    "      s_next = np.random.choice(env.states, p=env.transition_dist[s, a])\n",
    "      for i, r in enumerate(rewards):\n",
    "        episode_rewards[i].append(r[s, a, s_next])\n",
    "      s = s_next\n",
    "    \n",
    "    # at the end we compute the discounted return return\n",
    "    for r_i, r_values in enumerate(episode_rewards): # for each return func\n",
    "      return_val = 0 # accumulator for the return\n",
    "      for i, r in enumerate(r_values):\n",
    "        if i == 0: gamma_i = 1.0\n",
    "        else: gamma_i *= env.discount\n",
    "        return_val += gamma_i * r\n",
    "      return_vals[r_i].append(return_val)\n",
    "\n",
    "  return [sum(rs) / len(rs) for rs in return_vals]\n",
    "  \n",
    "def policy_return(reward: Reward, *args, **kwargs) -> Reward:\n",
    "  return policy_returns([reward], *args, **kwargs)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the policies we converge to are always the same\n",
    "for i in range(10):\n",
    "  e = RandomEnv()\n",
    "  r = random_reward(e)\n",
    "  policies = np.array([optimize(e, r) for _ in range(30)])\n",
    "  assert (policies == policies[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean=32.42517271610367 std=0.5659563702609113 dev=0.017454228392739884\n",
      "mean=10.76435090610558 std=0.14315140119765785 dev=0.013298656133224144\n",
      "mean=11.42376013264463 std=0.22046628957326722 dev=0.019298924961078354\n",
      "mean=87.68633601550484 std=1.2304171763801754 dev=0.014032028618034754\n",
      "mean=96.50735990152552 std=1.2189727385892422 dev=0.012630878513649751\n",
      "mean=50.916927273651886 std=0.719411359637228 dev=0.014129119688836834\n",
      "mean=191.39973099492278 std=1.5579669754479952 dev=0.008139859796821362\n",
      "mean=67.14360707532835 std=1.1409119076049759 dev=0.01699211521843007\n",
      "mean=83.88499562442665 std=0.5251940798961472 dev=0.0062608822470179015\n",
      "mean=3.962639062114664 std=0.7573540693541323 dev=0.19112365710894952\n",
      "mean=1 std=0.0 dev=0.0\n",
      "mean=3.3853230921944184 std=2.906851593632727e-06 dev=8.586629738045066e-07\n",
      "mean=10.977354849134247 std=0.1891860769315402 dev=0.017234213481443644\n",
      "mean=1 std=6.338430682868219e-06 dev=6.338430682868219e-06\n",
      "mean=1 std=3.6411193329891134e-06 dev=3.6411193329891134e-06\n",
      "mean=9.895089389053222 std=0.19563860880307551 dev=0.01977128261413256\n",
      "mean=57.80433043382246 std=0.1367432624720368 dev=0.002365623153936329\n",
      "mean=10.185898451974037 std=0.16846867589026449 dev=0.016539402654030495\n",
      "mean=1 std=0.0 dev=0.0\n",
      "mean=36.459774251964944 std=0.8504701596401698 dev=0.02332625961320468\n",
      "mean=118.33660141689991 std=1.3661079543793269 dev=0.011544255437643742\n",
      "mean=124.21843522789777 std=0.4161688362353757 dev=0.0033502984921026427\n",
      "mean=1 std=0.0 dev=0.0\n",
      "mean=93.62776275291837 std=0.752185662621674 dev=0.008033788702253578\n",
      "mean=88.1077118154344 std=1.3649119296688856 dev=0.015491401394330447\n",
      "mean=1 std=0.0 dev=0.0\n",
      "mean=1 std=3.6885102824015003e-07 dev=3.6885102824015003e-07\n",
      "mean=58.3348826946582 std=0.7044633701583246 dev=0.012076194167487942\n",
      "mean=72.21445189565985 std=1.2341983548988897 dev=0.01709073907646824\n",
      "mean=131.55346886463374 std=1.1399659739821497 dev=0.008665419344853272\n"
     ]
    }
   ],
   "source": [
    "# check that policy returns are reasonably low-variance\n",
    "deviations = []\n",
    "for _ in range(30):\n",
    "  e = RandomEnv()\n",
    "  r = random_reward(e)\n",
    "  policy = optimize(e, r)\n",
    "  Js = np.array([policy_return(r, policy, e)\n",
    "                  for _ in range(30)])\n",
    "  mean = Js.mean()\n",
    "  if abs(mean) < 1: mean = 1\n",
    "  std = Js.std()\n",
    "  dev = abs(std/mean)\n",
    "  print(f'{mean=} {std=} {dev=}')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=0\n",
      "i=50\n",
      "i=100\n",
      "i=150\n"
     ]
    }
   ],
   "source": [
    "# check that the best policy > random policy > worst policy\n",
    "for i in range(200):\n",
    "  if i % 50 == 0: print(f'{i=}')\n",
    "  e = RandomEnv()\n",
    "  r = random_reward(e)\n",
    "  best = optimize(e, r)\n",
    "  worst = optimize(e, -1*r)\n",
    "  random = np.random.randint(0, e.n_a-1, size=(e.n_s))\n",
    "  J_best = policy_return(r, best, e)\n",
    "  J_worst = policy_return(r, worst, e)\n",
    "  J_random = policy_return(r, random, e)\n",
    "\n",
    "  J_best_w_slack = J_best*1.01 if J_best > 0 else J_best*.99\n",
    "  J_random_w_slack = J_random*1.01 if J_random > 0 else J_random*.99\n",
    "  if J_best_w_slack < J_random or J_random_w_slack < J_worst:\n",
    "    print(i, J_best, J_random, J_worst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
