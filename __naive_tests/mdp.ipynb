{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "import abc\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax along last dimension\n",
    "def softmax(arr: np.ndarray) -> np.ndarray:\n",
    "  exp = np.exp(arr)\n",
    "  norm = np.sum(exp, axis=-1)\n",
    "  norm = np.reshape(norm, (*arr.shape[0:-1], 1))\n",
    "  norm = np.repeat(norm, arr.shape[-1], axis=-1)\n",
    "  return exp / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = int\n",
    "Action = int\n",
    "\n",
    "class MDP:\n",
    "  def __init__(\n",
    "    self,\n",
    "    states: list[State],\n",
    "    actions: list[Action],\n",
    "    discount_factor: float,\n",
    "    init_dist: np.ndarray,\n",
    "    transition_dist: np.ndarray, # [S, A, S']\n",
    "    rewards: np.ndarray, # [S, A, S']\n",
    "  ):\n",
    "    n_states = len(states)\n",
    "    n_actions = len(actions)\n",
    "\n",
    "    # check the dimensionality of the numpy arrays and that probs are valid\n",
    "    assert init_dist.shape == (n_states,)\n",
    "    assert init_dist.sum() == 1.0\n",
    "    assert transition_dist.shape == (n_states, n_actions, n_states)\n",
    "    assert np.isclose(transition_dist.sum(axis=-1), np.ones((n_states, n_actions))).all()\n",
    "    assert rewards.shape == (n_states, n_actions, n_states)\n",
    "\n",
    "    self.states = states\n",
    "    self.n_states = n_states\n",
    "    self.actions = actions\n",
    "    self.n_actions = n_actions\n",
    "    self.discount_factor = discount_factor\n",
    "    self.init_dist = init_dist\n",
    "    self.transition_dist = transition_dist\n",
    "    self.rewards = rewards\n",
    "\n",
    "    # start with zero Q-values and a uniform policy\n",
    "    self.q = np.zeros((self.n_states, self.n_actions))\n",
    "    self.policy = np.ones((self.n_states, self.n_actions)) / self.n_actions\n",
    "\n",
    "  def q_learn(\n",
    "    self,\n",
    "    max_iters=10000,\n",
    "    epsilon=0.1,\n",
    "    episode_len=100,\n",
    "    learning_rate=1e-3\n",
    "  ):\n",
    "    for i in range(max_iters):\n",
    "      # reset the episode every now and then so it doesn't get stuck\n",
    "      if i % episode_len == 0:\n",
    "        s = np.random.choice(self.states, p=self.init_dist)\n",
    "\n",
    "      # behavior policy = epsilon-greedy\n",
    "      if np.random.random() > epsilon:\n",
    "        a = self.q[s].argmax()\n",
    "      else:\n",
    "        a = np.random.choice(self.actions)\n",
    "\n",
    "      # sample next state\n",
    "      s_next = np.random.choice(self.states, p=self.transition_dist[s, a])\n",
    "      r = self.rewards[s, a, s_next]\n",
    "\n",
    "      # compute TD error and update Q value\n",
    "      delta = r + self.discount_factor * self.q[s_next].max() - self.q[s, a]\n",
    "      self.q[s, a] += learning_rate * delta\n",
    "\n",
    "      # the next state becomes the current state\n",
    "      s = s_next\n",
    "\n",
    "    # derive the policy by softmaxing along the actions dimension\n",
    "    self.policy = softmax(self.q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99891739, 0.00108261],\n",
       "       [0.01010049, 0.98989951],\n",
       "       [0.96327667, 0.03672333]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing MDP class\n",
    "mdp = MDP([0, 1, 2], [0, 1], 0.9,\n",
    "  init_dist=np.array([0.8, 0.1, 0.1]),\n",
    "  transition_dist=np.array([\n",
    "    [ # s=0\n",
    "      [0.1, 0.8, 0.1], # a=0\n",
    "      [0.1, 0.1, 0.8], # a=1\n",
    "    ],\n",
    "    [ # s=1\n",
    "      [0.1, 0.1, 0.8], # a=0\n",
    "      [0.8, 0.1, 0.1], # a=1\n",
    "    ],\n",
    "    [ # s=2\n",
    "      [0.8, 0.1, 0.1], # a=0\n",
    "      [0.1, 0.8, 0.1], # a=1\n",
    "    ],\n",
    "  ]),\n",
    "  rewards=np.array([\n",
    "    [ # s=0\n",
    "      [0, 5, -2], # a=0\n",
    "      [0, 5, -2], # a=1\n",
    "    ],\n",
    "    [ # s=1\n",
    "      [0, 1, -2], # a=0\n",
    "      [0, 1, -2], # a=1\n",
    "    ],\n",
    "    [ # s=2\n",
    "      [0, 30, 0], # a=0\n",
    "      [0, 30, 0], # a=1\n",
    "    ],\n",
    "  ])\n",
    ")\n",
    "mdp.q_learn()\n",
    "mdp.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32697404 0.22533149 0.22460465 0.22308982]\n",
      " [0.35319649 0.21521136 0.21594674 0.21564541]\n",
      " [0.34755482 0.2176787  0.2174106  0.21735588]\n",
      " [0.34887035 0.21880836 0.21553474 0.21678655]\n",
      " [0.3372785  0.21883396 0.22279749 0.22109005]\n",
      " [0.33701409 0.22135978 0.22097848 0.22064765]\n",
      " [0.32182285 0.22664733 0.22596094 0.22556888]\n",
      " [0.3428513  0.21851634 0.2197266  0.21890575]\n",
      " [0.33973438 0.21924564 0.22127436 0.21974561]\n",
      " [0.30696116 0.23028301 0.23012593 0.2326299 ]]\n",
      "[[0.3259688  0.2240804  0.22521227 0.22473853]\n",
      " [0.33935622 0.22084368 0.22023361 0.21956649]\n",
      " [0.34575425 0.2171866  0.21629388 0.22076528]\n",
      " [0.34179073 0.21863267 0.21909769 0.2204789 ]\n",
      " [0.31649603 0.22788084 0.22814348 0.22747966]\n",
      " [0.33254871 0.22151188 0.22274687 0.22319253]\n",
      " [0.31805146 0.22821878 0.2265468  0.22718297]\n",
      " [0.3454997  0.21764732 0.21936651 0.21748647]\n",
      " [0.34438323 0.21855957 0.21859761 0.21845959]\n",
      " [0.32073122 0.22628882 0.22592132 0.22705864]]\n"
     ]
    }
   ],
   "source": [
    "n_s = 10\n",
    "states = [i for i in range(n_s)]\n",
    "n_a = 4\n",
    "actions = [i for i in range(n_a)]\n",
    "r1 = np.random.random((n_s, n_a, n_s))\n",
    "r2 = np.random.random((n_s, n_a, n_s))\n",
    "transitions = softmax(np.random.random((n_s, n_a, n_s)))\n",
    "init_dist = softmax(np.random.random(n_s))\n",
    "\n",
    "mdp1 = MDP(states, actions, 0.9, init_dist, transitions, r1)\n",
    "mdp1.q_learn()\n",
    "mdp2 = MDP(states, actions, 0.9, init_dist, transitions, r2)\n",
    "mdp2.q_learn()\n",
    "print(mdp1.policy)\n",
    "print(mdp2.policy)\n",
    "\n",
    "# TODO: compare regrets with rollouts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "178f5955e7ec2db83c487531b6f19a6ba078d2c9bcad0eaf79872b0fcb34bd80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
