{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from env import Env, RandomEnv\n",
    "from reward import random_reward\n",
    "from _types import Reward\n",
    "from utils import timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def minimal_canonx(reward: Reward, env: Env, norm_ord: int) -> Reward:\n",
    "    r = torch.tensor(reward)\n",
    "    potential = torch.zeros(env.n_s, requires_grad=True)\n",
    "    optimizer = optimizer = torch.optim.AdamW([potential], lr=1e-3)\n",
    "\n",
    "    def smooth_inf_norm(x, alpha):\n",
    "        return (1 / alpha) * torch.logsumexp(alpha * x, dim=tuple(range(x.ndim)))\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        r_prime = r + env.discount * potential[None, None, :] - potential[:, None, None]\n",
    "        alpha = 1.0\n",
    "        loss = smooth_inf_norm(r_prime, alpha)\n",
    "        loss.backward()\n",
    "        return loss, r_prime\n",
    "\n",
    "    for i in range(200000):\n",
    "        optimizer.step()\n",
    "\n",
    "        loss, r_prime = closure()\n",
    "        loss = loss.item()\n",
    "        grad_norm = torch.norm(potential.grad, 2)\n",
    "\n",
    "        if grad_norm < 1e-4: break\n",
    "        if i %1000== 0: print(f\"Initial {norm_ord=} loss={loss} grad={grad_norm}\")\n",
    "        if i == 200000-1: print(f\"Didn't converge {norm_ord=} loss={loss} grad={grad_norm}\")\n",
    "\n",
    "    return r_prime.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def minimal_canon(reward: Reward, env: Env, norm_ord: int) -> Reward:\n",
    "  r = torch.tensor(reward)\n",
    "  # potential = torch.tensor(reward.mean(axis=(1, 2)), requires_grad=True)\n",
    "  potential = torch.zeros(env.n_s, requires_grad=True)\n",
    "  optimizer = torch.optim.Adam([potential], lr = 1e-2)\n",
    "  for i in range(200000):\n",
    "    optimizer.zero_grad()\n",
    "    r_prime = r + env.discount * potential[None, None, :] - potential[:, None, None]\n",
    "    loss = torch.norm(r_prime, norm_ord)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if torch.norm(potential.grad, 2) < 1e-4: break\n",
    "    if i == 0: print(f\"Initial {norm_ord=} loss={loss.item()} grad={torch.norm(potential.grad, 2)}\")\n",
    "    if i %1000== 0: print(f\"Running {norm_ord=} loss={loss} grad={torch.norm(potential.grad, 2)}\")\n",
    "    if i == 200000-1: print(f\"Didn't converge {norm_ord=} loss={loss.item()} grad={torch.norm(potential.grad, 2)}\")\n",
    "  return r_prime.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_ord=inf\n",
      "Initial norm_ord=inf loss=41.5733948707029 grad=0.9245467782020569\n",
      "Initial norm_ord=inf loss=39.68673829583661 grad=0.9219754338264465\n",
      "Initial norm_ord=inf loss=37.82967453155023 grad=0.9138655066490173\n",
      "Initial norm_ord=inf loss=36.04899127575182 grad=0.8757125735282898\n",
      "Initial norm_ord=inf loss=34.4831486998651 grad=0.75460284948349\n",
      "Initial norm_ord=inf loss=33.4046149900243 grad=0.5120911002159119\n",
      "Initial norm_ord=inf loss=32.90994761315389 grad=0.2743326723575592\n",
      "Initial norm_ord=inf loss=32.724756265647265 grad=0.14148031175136566\n",
      "Initial norm_ord=inf loss=32.644359168453946 grad=0.08058717846870422\n",
      "Initial norm_ord=inf loss=32.59584921618627 grad=0.05616192892193794\n",
      "Initial norm_ord=inf loss=32.551106444600514 grad=0.04879264906048775\n",
      "Initial norm_ord=inf loss=32.49586263023192 grad=0.047331809997558594\n",
      "Initial norm_ord=inf loss=32.42518276395282 grad=0.047433044761419296\n",
      "Initial norm_ord=inf loss=32.3425120840166 grad=0.047866810113191605\n",
      "Initial norm_ord=inf loss=32.25374162544261 grad=0.04842782020568848\n",
      "Initial norm_ord=inf loss=32.16291781161204 grad=0.049059610813856125\n",
      "Initial norm_ord=inf loss=32.07197194289791 grad=0.04972776770591736\n",
      "Initial norm_ord=inf loss=31.981676199383507 grad=0.05040951073169708\n",
      "Initial norm_ord=inf loss=31.89234183536475 grad=0.051100317388772964\n",
      "Initial norm_ord=inf loss=31.80403464196666 grad=0.051778644323349\n",
      "Initial norm_ord=inf loss=31.71678352061946 grad=0.052444521337747574\n",
      "Initial norm_ord=inf loss=31.63058781505153 grad=0.05308898910880089\n",
      "Initial norm_ord=inf loss=31.54542969827915 grad=0.053710635751485825\n",
      "Initial norm_ord=inf loss=31.461279634981832 grad=0.05429888516664505\n",
      "Initial norm_ord=inf loss=31.378106976297964 grad=0.05485367029905319\n",
      "Initial norm_ord=inf loss=31.29592184247657 grad=0.05537733435630798\n",
      "Initial norm_ord=inf loss=31.214686225955734 grad=0.055871300399303436\n",
      "Initial norm_ord=inf loss=31.13435586953925 grad=0.0563168078660965\n",
      "Initial norm_ord=inf loss=31.054964044093108 grad=0.056743089109659195\n",
      "Initial norm_ord=inf loss=30.976446004899163 grad=0.05712107568979263\n",
      "Initial norm_ord=inf loss=30.898798205985287 grad=0.0574801079928875\n",
      "Initial norm_ord=inf loss=30.82204247554171 grad=0.05779632180929184\n",
      "Initial norm_ord=inf loss=30.746073063028742 grad=0.05808378756046295\n",
      "Initial norm_ord=inf loss=30.67098559171212 grad=0.0583542101085186\n",
      "Initial norm_ord=inf loss=30.59670823745908 grad=0.05858632177114487\n",
      "Initial norm_ord=inf loss=30.523191673552212 grad=0.05879300460219383\n",
      "Initial norm_ord=inf loss=30.450476357870702 grad=0.058985281735658646\n",
      "Initial norm_ord=inf loss=30.378556073726916 grad=0.05915829539299011\n",
      "Initial norm_ord=inf loss=30.307393466259914 grad=0.059306658804416656\n",
      "Initial norm_ord=inf loss=30.236947203661543 grad=0.05943404138088226\n",
      "Initial norm_ord=inf loss=30.167263764107023 grad=0.059551138430833817\n",
      "Initial norm_ord=inf loss=30.098292569916556 grad=0.05965308099985123\n",
      "Initial norm_ord=inf loss=30.03001931220377 grad=0.059740204364061356\n",
      "Initial norm_ord=inf loss=29.962432725996077 grad=0.05981763079762459\n",
      "Initial norm_ord=inf loss=29.89550915261012 grad=0.059883203357458115\n",
      "Initial norm_ord=inf loss=29.829248175146784 grad=0.05993752181529999\n",
      "Initial norm_ord=inf loss=29.763677980993062 grad=0.059982649981975555\n",
      "Initial norm_ord=inf loss=29.698748815974092 grad=0.060022030025720596\n",
      "Initial norm_ord=inf loss=29.63444973013444 grad=0.06005316600203514\n",
      "Initial norm_ord=inf loss=29.570830629571265 grad=0.06007549166679382\n",
      "Initial norm_ord=inf loss=29.507875526999797 grad=0.06009497120976448\n",
      "Initial norm_ord=inf loss=29.445537616319648 grad=0.06010795757174492\n",
      "Initial norm_ord=inf loss=29.383862843055386 grad=0.06011366471648216\n",
      "Initial norm_ord=inf loss=29.322816274078583 grad=0.06011772155761719\n",
      "Initial norm_ord=inf loss=29.26235542246945 grad=0.060117073357105255\n",
      "Initial norm_ord=inf loss=29.202560932040367 grad=0.060115743428468704\n",
      "Initial norm_ord=inf loss=29.14332549871464 grad=0.06011208891868591\n",
      "Initial norm_ord=inf loss=29.08482530541131 grad=0.060108285397291183\n",
      "Initial norm_ord=inf loss=29.02695947653489 grad=0.06009892374277115\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000019?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m n_ord \u001b[39min\u001b[39;00m [np\u001b[39m.\u001b[39minf]:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000019?line=4'>5</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mn_ord\u001b[39m=}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000019?line=5'>6</a>\u001b[0m   canon1, canon2 \u001b[39m=\u001b[39m minimal_canonx(r, e, n_ord)\n",
      "File \u001b[0;32m~/Documents/epic_like_distances_draft/utils.py:17\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/sumeet/Documents/epic_like_distances_draft/utils.py?line=13'>14</a>\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m     <a href='file:///Users/sumeet/Documents/epic_like_distances_draft/utils.py?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     <a href='file:///Users/sumeet/Documents/epic_like_distances_draft/utils.py?line=15'>16</a>\u001b[0m   st \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m---> <a href='file:///Users/sumeet/Documents/epic_like_distances_draft/utils.py?line=16'>17</a>\u001b[0m   out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///Users/sumeet/Documents/epic_like_distances_draft/utils.py?line=17'>18</a>\u001b[0m   et \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m     <a href='file:///Users/sumeet/Documents/epic_like_distances_draft/utils.py?line=18'>19</a>\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m took \u001b[39m\u001b[39m{\u001b[39;00met\u001b[39m-\u001b[39mst\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb Cell 2'\u001b[0m in \u001b[0;36mminimal_canonx\u001b[0;34m(reward, env, norm_ord)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000006?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m200000\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000006?line=18'>19</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000006?line=20'>21</a>\u001b[0m     loss, r_prime \u001b[39m=\u001b[39m closure()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000006?line=21'>22</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000006?line=22'>23</a>\u001b[0m     grad_norm \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(potential\u001b[39m.\u001b[39mgrad, \u001b[39m2\u001b[39m)\n",
      "\u001b[1;32m/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb Cell 2'\u001b[0m in \u001b[0;36mminimal_canonx.<locals>.closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000006?line=12'>13</a>\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000006?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m smooth_inf_norm(r_prime, alpha)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000006?line=14'>15</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sumeet/Documents/epic_like_distances_draft/minimal.ipynb#ch0000006?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss, r_prime\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=386'>387</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=387'>388</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=388'>389</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=389'>390</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=393'>394</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=394'>395</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/_tensor.py?line=395'>396</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(1):\n",
    "  e = RandomEnv(n_s=32, n_a=8)\n",
    "  r = random_reward(e)\n",
    "  for n_ord in [np.inf]:\n",
    "    print(f'{n_ord=}')\n",
    "    canon1, canon2 = minimal_canonx(r, e, n_ord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
  },
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
