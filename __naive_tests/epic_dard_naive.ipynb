{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import typing\n",
    "import abc\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some utils\n",
    "\n",
    "# softmax along last dimension\n",
    "def softmax(arr: np.ndarray) -> np.ndarray:\n",
    "  exp = np.exp(arr)\n",
    "  norm = np.sum(exp, axis=-1)\n",
    "  norm = np.reshape(norm, (*arr.shape[0:-1], 1))\n",
    "  norm = np.repeat(norm, arr.shape[-1], axis=-1)\n",
    "  return exp / norm\n",
    "\n",
    "Reward = np.ndarray\n",
    "\n",
    "def l1_norm(reward: Reward) -> float:\n",
    "  return np.abs(reward).sum()\n",
    "\n",
    "def l2_norm(reward: Reward) -> float:\n",
    "  out = np.power(reward, 2).sum()\n",
    "  return np.sqrt(out)\n",
    "\n",
    "def l_infty_norm(reward: Reward) -> float:\n",
    "  return np.abs(reward).max()\n",
    "\n",
    "def random_dist(n):\n",
    "  logits = np.random.random(n)\n",
    "  exp = np.exp(logits)\n",
    "  return exp / sum(exp)\n",
    "\n",
    "def random_reward(n_s, n_a):\n",
    "  return np.random.random((n_s, n_a, n_s))\n",
    "\n",
    "def random_transition_dist(n_s, n_a):\n",
    "  return softmax(np.random.random((n_s, n_a, n_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardDistance():\n",
    "  def __init__(self, discount=0.9) -> None:\n",
    "    self.discount = discount\n",
    "\n",
    "  @abc.abstractmethod\n",
    "  def canonicalize(self, reward: Reward) -> Reward:\n",
    "    pass\n",
    "\n",
    "  def normalize(self, reward: Reward) -> float:\n",
    "    return l2_norm(reward)\n",
    "\n",
    "  def distance(self, r1: Reward, r2: Reward) -> float:\n",
    "    return l2_norm(r1 - r2)\n",
    "\n",
    "  def __call__(self, r1: Reward, r2: Reward) -> float:\n",
    "    can1 = self.canonicalize(r1)\n",
    "    can2 = self.canonicalize(r2)\n",
    "\n",
    "    standard1 = can1 / self.normalize(can1)\n",
    "    standard2 = can2 / self.normalize(can2)\n",
    "\n",
    "    return self.distance(standard1, standard2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3935490871336487"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Epic(RewardDistance):\n",
    "  def __init__(self, state_dist: np.ndarray, action_dist: np.ndarray):\n",
    "    super().__init__()\n",
    "    self.state_dist = state_dist\n",
    "    self.action_dist = action_dist\n",
    "    \n",
    "  def canonicalize(self, reward: Reward) -> Reward:\n",
    "    result = copy.deepcopy(reward)\n",
    "\n",
    "    # memoize all the expected values\n",
    "    expected_term1 = [] # depends on s_prime\n",
    "    for s_prime in range(len(self.state_dist)):\n",
    "      ev1 = 0\n",
    "\n",
    "      for A, A_prob in enumerate(self.action_dist):\n",
    "        for S_prime, S_prime_prob in enumerate(self.state_dist):\n",
    "          prob = A_prob * S_prime_prob\n",
    "          ev1 += prob * self.discount * reward[s_prime, A, S_prime]\n",
    "\n",
    "      expected_term1.append(ev1)\n",
    "  \n",
    "    expected_term2 = [] # depends on s\n",
    "    for s in range(len(self.state_dist)):\n",
    "      ev2 = 0\n",
    "\n",
    "      for A, A_prob in enumerate(self.action_dist):\n",
    "        for S_prime, S_prime_prob in enumerate(self.state_dist):\n",
    "          prob = A_prob * S_prime_prob\n",
    "          ev2 += prob * self.discount * reward[s, A, S_prime]\n",
    "\n",
    "      expected_term2.append(ev2)\n",
    "\n",
    "    expected_term3 = 0 # constant across all state values\n",
    "    for S, S_prob in enumerate(self.state_dist):\n",
    "      for A, A_prob in enumerate(self.action_dist):\n",
    "        for S_prime, S_prime_prob in enumerate(self.state_dist):\n",
    "          prob = S_prob * A_prob * S_prime_prob\n",
    "          expected_term3 += prob * self.discount * reward[S, A, S_prime]\n",
    "\n",
    "\n",
    "    for s in range(len(self.state_dist)):\n",
    "      for a in range(len(self.action_dist)):\n",
    "        for s_prime in range(len(self.state_dist)):\n",
    "          terms = expected_term1[s_prime] - expected_term2[s] - expected_term3\n",
    "          result[s, a, s_prime] += terms\n",
    "        \n",
    "    return result\n",
    "  \n",
    "\n",
    "# test EPIC\n",
    "n_states = 200\n",
    "n_actions = 10\n",
    "state_dist = random_dist(n_states)\n",
    "action_dist = random_dist(n_actions)\n",
    "r1 = random_reward(n_states, n_actions)\n",
    "r2 = random_reward(n_states, n_actions)\n",
    "Epic(state_dist, action_dist)(r1, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.393991041812696"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Dard(RewardDistance):\n",
    "  def __init__(\n",
    "    self,\n",
    "    state_dist: np.ndarray,\n",
    "    action_dist: np.ndarray,\n",
    "    transition_dist: np.ndarray,\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.state_dist = state_dist\n",
    "    self.action_dist = action_dist\n",
    "    self.transition_dist = transition_dist\n",
    "    \n",
    "  def canonicalize(self, reward: Reward) -> Reward:\n",
    "    result = copy.deepcopy(reward)\n",
    "\n",
    "    # memoize all the expected values\n",
    "    expected_term1 = [] # depends on s_prime\n",
    "    for s_prime in range(len(self.state_dist)):\n",
    "      ev1 = 0\n",
    "\n",
    "      for A, A_prob in enumerate(self.action_dist):\n",
    "        for S_double, S_double_prob in enumerate(self.transition_dist[s_prime, A, :]):\n",
    "          prob = A_prob * S_double_prob\n",
    "          ev1 += prob * self.discount * reward[s_prime, A, S_double]\n",
    "\n",
    "      expected_term1.append(ev1)\n",
    "  \n",
    "    expected_term2 = [] # depends on s\n",
    "    for s in range(len(self.state_dist)):\n",
    "      ev2 = 0\n",
    "\n",
    "      for A, A_prob in enumerate(self.action_dist):\n",
    "        for S_prime, S_prime_prob in enumerate(self.state_dist):\n",
    "          prob = A_prob * S_prime_prob\n",
    "          ev2 += prob * self.discount * reward[s, A, S_prime]\n",
    "\n",
    "      expected_term2.append(ev2)\n",
    "\n",
    "    expected_term3 = 0 # constant across all state values\n",
    "    for A, A_prob in enumerate(self.action_dist):\n",
    "      for S_prime, S_prime_prob in enumerate(self.transition_dist[s, A, :]):\n",
    "        for S_double, S_double_prob in enumerate(self.transition_dist[s_prime, A, :]):\n",
    "          prob = S_prime_prob * A_prob * S_double_prob\n",
    "          expected_term3 += prob * self.discount * reward[S_prime, A, S_double]\n",
    "\n",
    "\n",
    "    for s in range(len(self.state_dist)):\n",
    "      for a in range(len(self.action_dist)):\n",
    "        for s_prime in range(len(self.state_dist)):\n",
    "          terms = expected_term1[s_prime] - expected_term2[s] - expected_term3\n",
    "          result[s, a, s_prime] += terms\n",
    "        \n",
    "    return result\n",
    "\n",
    "    \n",
    "# test DARD\n",
    "n_states = 200\n",
    "n_actions = 10\n",
    "state_dist = random_dist(n_states)\n",
    "action_dist = random_dist(n_actions)\n",
    "transition_dist = random_transition_dist(n_states, n_actions)\n",
    "r1 = random_reward(n_states, n_actions)\n",
    "r2 = random_reward(n_states, n_actions)\n",
    "Dard(state_dist, action_dist, transition_dist)(r1, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPIC=1.39686, DARD=1.39719, difference=0.00033\n",
      "EPIC=1.39438, DARD=1.39478, difference=0.00040\n",
      "EPIC=1.39713, DARD=1.39715, difference=0.00001\n",
      "EPIC=1.39500, DARD=1.39482, difference=0.00018\n",
      "EPIC=1.38952, DARD=1.38961, difference=0.00009\n",
      "EPIC=1.39349, DARD=1.39341, difference=0.00008\n",
      "EPIC=1.39231, DARD=1.39207, difference=0.00024\n",
      "EPIC=1.39334, DARD=1.39352, difference=0.00018\n",
      "EPIC=1.39521, DARD=1.39541, difference=0.00019\n",
      "EPIC=1.39460, DARD=1.39438, difference=0.00022\n"
     ]
    }
   ],
   "source": [
    "# compare them\n",
    "n_states = 100\n",
    "n_actions = 10\n",
    "state_dist = random_dist(n_states)\n",
    "action_dist = random_dist(n_actions)\n",
    "transition_dist = random_transition_dist(n_states, n_actions)\n",
    "\n",
    "epic = Epic(state_dist, action_dist) \n",
    "dard = Dard(state_dist, action_dist, transition_dist)\n",
    "\n",
    "for _ in range(10):\n",
    "  r1 = random_reward(n_states, n_actions)\n",
    "  r2 = random_reward(n_states, n_actions)\n",
    "  epic_result = epic(r1, r2)\n",
    "  dard_result = dard(r1, r2)\n",
    "  diff = abs(epic_result-dard_result)\n",
    "  print(f\"EPIC={epic_result:.5f}, DARD={dard_result:.5f}, difference={diff:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "178f5955e7ec2db83c487531b6f19a6ba078d2c9bcad0eaf79872b0fcb34bd80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
