{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from env import Env, RandomEnv\n",
    "from reward import random_reward\n",
    "from _types import Reward\n",
    "from utils import timed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timed\n",
    "def minimal_canon(reward: Reward, env: Env, norm_ord: int|float, steps=100000) -> Reward:\n",
    "  r = torch.tensor(reward)\n",
    "  potential = torch.tensor(reward.mean(axis=(1, 2)), requires_grad=True)\n",
    "  # potential = torch.zeros(env.n_s, requires_grad=True)\n",
    "  frozen_potential = torch.clone(potential)  \n",
    "\n",
    "  optimizer = torch.optim.Adam([potential], lr=1e-5)\n",
    "  for i in range(steps):\n",
    "    optimizer.zero_grad()\n",
    "    r_prime = r + env.discount * potential[None, None, :] - potential[:, None, None]\n",
    "    loss = torch.norm(r_prime, norm_ord)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # convergence = small gradient or potential hasn't changed in a while\n",
    "    if torch.norm(potential.grad, 2) < 1e-4:\n",
    "      print(f\"Converged by low grad at {i=}\")\n",
    "      break\n",
    "    if i%10000 == 0 and i != 0:\n",
    "      if torch.isclose(potential, frozen_potential, rtol=1e-3, atol=1e-3).all():\n",
    "        print(f\"Converged by no movement {i=}\")\n",
    "        break\n",
    "      else: frozen_potential = torch.clone(potential)\n",
    "\n",
    "    if i == 0: print(f\"Initial {norm_ord=} loss={loss.item()} grad={torch.norm(potential.grad, 2)}\")\n",
    "    if i == steps-1: print(f\"Didn't converge {norm_ord=} loss={loss.item()} grad={torch.norm(potential.grad, 2)}\")\n",
    "    if i%10000 == 0: print(f'{i=} grad norm={torch.norm(potential.grad, 2)}')\n",
    "    # if i%10000 == 0: print(f'{potential=}\\n\\n{potential.grad=}\\n\\ngrad norm={torch.norm(potential.grad, 2)}\\n\\n\\n\\n')\n",
    "  return r_prime.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial norm_ord=1 loss=6642.052112415129 grad=120.64990675504062\n",
      "i=0 grad norm=120.64990675504062\n",
      "i=10000 grad norm=7.211102550927978\n",
      "Converged by no movement i=20000\n",
      "minimal_canon took 3.3108s\n"
     ]
    }
   ],
   "source": [
    "e = RandomEnv(n_s=32, n_a=8)\n",
    "r = random_reward(e)\n",
    "_ = minimal_canon(r, e, 1, steps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_ord=1\n",
      "Initial norm_ord=1 loss=74605.61017625945 grad=144.8155059814453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m n_ord \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)]:\n\u001b[1;32m      5\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mn_ord\u001b[39m=}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m   canon1 \u001b[39m=\u001b[39m minimal_canon(r, e, n_ord)\n\u001b[1;32m      7\u001b[0m   canon2 \u001b[39m=\u001b[39m minimal_canon(r, e, n_ord)\n\u001b[1;32m      8\u001b[0m   \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39misclose(canon1, canon2)\u001b[39m.\u001b[39mall()\n",
      "File \u001b[0;32m~/Desktop/Research/Reward Distance/utils.py:17\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     16\u001b[0m   st \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m---> 17\u001b[0m   out \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     18\u001b[0m   et \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m     19\u001b[0m   \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m took \u001b[39m\u001b[39m{\u001b[39;00met\u001b[39m-\u001b[39mst\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mminimal_canon\u001b[0;34m(reward, env, norm_ord, steps)\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam([potential])\n\u001b[1;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(steps):\n\u001b[0;32m----> 8\u001b[0m   optimizer\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m      9\u001b[0m   r_prime \u001b[39m=\u001b[39m r \u001b[39m+\u001b[39m env\u001b[39m.\u001b[39mdiscount \u001b[39m*\u001b[39m potential[\u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, :] \u001b[39m-\u001b[39m potential[:, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     10\u001b[0m   loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnorm(r_prime, norm_ord)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/torch/optim/optimizer.py:267\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m foreach:\n\u001b[1;32m    266\u001b[0m     per_device_and_dtype_grads \u001b[39m=\u001b[39m defaultdict(\u001b[39mlambda\u001b[39;00m: defaultdict(\u001b[39mlist\u001b[39m))\n\u001b[0;32m--> 267\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    268\u001b[0m     \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[1;32m    269\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/torch/autograd/profiler.py:493\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any):\n\u001b[1;32m    492\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks_on_exit:\n\u001b[0;32m--> 493\u001b[0m         torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mprofiler\u001b[39m.\u001b[39;49m_record_function_exit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch/lib/python3.10/site-packages/torch/_ops.py:442\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    438\u001b[0m     \u001b[39m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    439\u001b[0m     \u001b[39m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[39m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[39m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_op(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs \u001b[39mor\u001b[39;49;00m {})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "  e = RandomEnv(n_s=32, n_a=8)\n",
    "  r = random_reward(e)\n",
    "  for n_ord in [1, 2, float('inf')]:\n",
    "    print(f'{n_ord=}')\n",
    "    canon1 = minimal_canon(r, e, n_ord)\n",
    "    canon2 = minimal_canon(r, e, n_ord)\n",
    "    assert np.isclose(canon1, canon2).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
