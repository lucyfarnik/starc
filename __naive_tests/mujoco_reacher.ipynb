{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/users/motwani/conda/envs/cs285/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import abc\n",
    "from typing import Optional\n",
    "\n",
    "import gym\n",
    "import torch\n",
    "\n",
    "\n",
    "class RewardModel(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def reward(\n",
    "            self,\n",
    "            states: torch.Tensor,\n",
    "            actions: torch.Tensor,\n",
    "            next_states: Optional[torch.Tensor],\n",
    "            terminals: Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Computes the reward for the associated transitions.\n",
    "\n",
    "        We assume that all reward models operate on `torch.Tensor`s.\n",
    "\n",
    "        Args:\n",
    "            states: The states.\n",
    "            actions: The actions.\n",
    "            next_states: The next states. Some reward models don't use these so they're optional.\n",
    "            terminals: Indicators for whether the transition ended an episode.\n",
    "                Some reward models don't use these so they're optional.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of scalar reward values.\n",
    "        \"\"\"\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def observation_space(self) -> gym.spaces.Space:\n",
    "        \"\"\"Returns the observation space of this reward model.\"\"\"\n",
    "    @property\n",
    "    @abc.abstractmethod\n",
    "    def action_space(self) -> gym.spaces.Space:\n",
    "        \"\"\"Returns the action space of this reward model.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import gym\n",
    "from gym.envs.mujoco.reacher import ReacherEnv\n",
    "import numpy as np\n",
    "import torch\n",
    "import mujoco_py\n",
    "\n",
    "\n",
    "class CustomReacherEnv(ReacherEnv):\n",
    "    \"\"\"A customized version of the reacher env.\n",
    "\n",
    "    Customization includes frame skip, changing the obs to allow for simulation from it,\n",
    "    making the info dict json serializable, setting a finite horizon independent of the\n",
    "    gym wrapper for doing so, and other changes.\n",
    "\n",
    "    Args:\n",
    "        frame_skip: Number of frames to skip between timesteps.\n",
    "        max_timesteps: The maximum number of timesteps to take in the env per episode.\n",
    "        obs_mode: The mode for the obseravtion. Options:\n",
    "            sim: Returns an observation that allows for simulating the mujoco simulator (default).\n",
    "            original: Returns the original observation from the environment.\n",
    "        terminate_when_unhealthy: If True, terminates the episode when healthy state bounds are exceeded.\n",
    "        healthy_velocity_range: Tuple of min/max velocity values that define healthy bounds.\n",
    "            These exist because without them rllib sometimes errors out with nan gradients when there are\n",
    "            very large velocity values.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            frame_skip: int = 5,\n",
    "            max_timesteps: int = 100,\n",
    "            obs_mode: str = \"sim\",\n",
    "            terminate_when_unhealthy: bool = True,\n",
    "            healthy_velocity_range: Tuple[int, int] = (-50, 50),\n",
    "    ):\n",
    "        # These have to be stored before super init b/c it calls step.\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.t = 0\n",
    "        self.obs_mode = obs_mode\n",
    "        self.terminate_when_unhealthy = terminate_when_unhealthy\n",
    "        self.healthy_velocity_range = healthy_velocity_range\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Overwrite frame skip after calling super init.\n",
    "        self.frame_skip = frame_skip\n",
    "        self.metadata[\"video.frames_per_second\"] = int(np.round(1.0 / self.dt))\n",
    "\n",
    "    def is_healthy(self) -> bool:\n",
    "        \"\"\"Returns True if the simulator is in a healthy state.\"\"\"\n",
    "        min_velocity, max_velocity = self.healthy_velocity_range\n",
    "        velocity = self.sim.data.qvel.flat[:]\n",
    "        healthy_velocity = np.all(np.logical_and(min_velocity < velocity, velocity < max_velocity))\n",
    "\n",
    "        healthy = healthy_velocity\n",
    "        return healthy\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"Resets the environment.\"\"\"\n",
    "        self.t = 0\n",
    "        return super().reset()\n",
    "\n",
    "    def step(self, *args, **kwargs) -> Tuple:\n",
    "        \"\"\"Fixes a non-json-writable element in the info of the base env.\"\"\"\n",
    "        obs, reward, done, info = super().step(*args, **kwargs)\n",
    "        info[\"reward_ctrl\"] = float(info[\"reward_ctrl\"])\n",
    "\n",
    "        if self.terminate_when_unhealthy and not self.is_healthy():\n",
    "            done = True\n",
    "\n",
    "        self.t += 1\n",
    "        if self.t >= self.max_timesteps:\n",
    "            done = True\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        \"\"\"Optionally overwrite the observation to for simulation purposes.\"\"\"\n",
    "        if self.obs_mode == \"sim\":\n",
    "            return np.concatenate([\n",
    "                self.sim.data.qpos.flat[:],\n",
    "                self.sim.data.qvel.flat[:],\n",
    "                self.get_body_com(\"fingertip\") - self.get_body_com(\"target\"),\n",
    "            ])\n",
    "        elif self.obs_mode == \"original\":\n",
    "            return super()._get_obs()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid observation mode: {self.obs_mode}\")\n",
    "\n",
    "\n",
    "class CustomReacherEnvRewardModel(RewardModel):\n",
    "    \"\"\"Reward model for custom Reacher environment.\n",
    "\n",
    "    Args:\n",
    "        obs_space: The observation space used in the environment.\n",
    "        act_space: The action space used in the environment.\n",
    "        reward_dist_factor: Weight on the distance from goal reward term.\n",
    "        reward_ctrl_factor: Weight on the control reward term.\n",
    "        reward_goal_factor: Weight on reaching the goal.\n",
    "        shaping_factor: The value to scale the shaping.\n",
    "        shaping_discount: The discount factor used in potential shaping.\n",
    "    \"\"\"\n",
    "    # At this threshold around 2% of initial states are next to the goal.\n",
    "    GOAL_REACHED_THRESHOLD = 0.05\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            obs_space: gym.spaces.Space,\n",
    "            act_space: gym.spaces.Space,\n",
    "            reward_dist_factor: float,\n",
    "            reward_ctrl_factor: float,\n",
    "            reward_goal_factor: float,\n",
    "            shaping_factor: float,\n",
    "            shaping_discount: float,\n",
    "    ):\n",
    "        self.obs_space = obs_space\n",
    "        self.act_space = act_space\n",
    "        self.reward_dist_factor = reward_dist_factor\n",
    "        self.reward_ctrl_factor = reward_ctrl_factor\n",
    "        self.reward_goal_factor = reward_goal_factor\n",
    "        self.shaping_factor = shaping_factor\n",
    "        self.shaping_discount = shaping_discount\n",
    "\n",
    "    @property\n",
    "    def observation_space(self) -> gym.spaces.Space:\n",
    "        return self.obs_space\n",
    "\n",
    "    @property\n",
    "    def action_space(self) -> gym.spaces.Space:\n",
    "        return self.act_space\n",
    "\n",
    "    def reward(\n",
    "            self,\n",
    "            states: torch.Tensor,\n",
    "            actions: torch.Tensor,\n",
    "            next_states: Optional[torch.Tensor],\n",
    "            terminals: Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Computes the reward for the environment.\n",
    "\n",
    "        See base class for documentation on args and return value.\n",
    "        \"\"\"\n",
    "        del terminals\n",
    "        states_dists = states[:, -3:].norm(dim=-1, keepdim=True)\n",
    "        dist_rewards = -states_dists\n",
    "        ctrl_rewards = -actions.square().sum(dim=1, keepdim=True).to(states.dtype)\n",
    "        goal_rewards = states_dists < self.GOAL_REACHED_THRESHOLD\n",
    "\n",
    "        next_states_dists = next_states[:, -3:].norm(dim=-1, keepdim=True)\n",
    "        shaping_rewards = (self.shaping_discount * next_states_dists - states_dists)\n",
    "\n",
    "        rewards = self.reward_dist_factor * dist_rewards \\\n",
    "            + self.reward_ctrl_factor * ctrl_rewards \\\n",
    "            + self.reward_goal_factor * goal_rewards \\\n",
    "            + self.shaping_factor * shaping_rewards\n",
    "\n",
    "        return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "\n",
    "import gym\n",
    "from gym.envs.mujoco.reacher import ReacherEnv\n",
    "import numpy as np\n",
    "import torch\n",
    "import mujoco_py\n",
    "\n",
    "\n",
    "class CustomReacherEnv(ReacherEnv):\n",
    "    \"\"\"A customized version of the reacher env.\n",
    "\n",
    "    Customization includes frame skip, changing the obs to allow for simulation from it,\n",
    "    making the info dict json serializable, setting a finite horizon independent of the\n",
    "    gym wrapper for doing so, and other changes.\n",
    "\n",
    "    Args:\n",
    "        frame_skip: Number of frames to skip between timesteps.\n",
    "        max_timesteps: The maximum number of timesteps to take in the env per episode.\n",
    "        obs_mode: The mode for the obseravtion. Options:\n",
    "            sim: Returns an observation that allows for simulating the mujoco simulator (default).\n",
    "            original: Returns the original observation from the environment.\n",
    "        terminate_when_unhealthy: If True, terminates the episode when healthy state bounds are exceeded.\n",
    "        healthy_velocity_range: Tuple of min/max velocity values that define healthy bounds.\n",
    "            These exist because without them rllib sometimes errors out with nan gradients when there are\n",
    "            very large velocity values.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            frame_skip: int = 5,\n",
    "            max_timesteps: int = 100,\n",
    "            obs_mode: str = \"sim\",\n",
    "            terminate_when_unhealthy: bool = True,\n",
    "            healthy_velocity_range: Tuple[int, int] = (-50, 50),\n",
    "    ):\n",
    "        # These have to be stored before super init b/c it calls step.\n",
    "        self.max_timesteps = max_timesteps\n",
    "        self.t = 0\n",
    "        self.obs_mode = obs_mode\n",
    "        self.terminate_when_unhealthy = terminate_when_unhealthy\n",
    "        self.healthy_velocity_range = healthy_velocity_range\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Overwrite frame skip after calling super init.\n",
    "        self.frame_skip = frame_skip\n",
    "        self.metadata[\"video.frames_per_second\"] = int(np.round(1.0 / self.dt))\n",
    "\n",
    "    def is_healthy(self) -> bool:\n",
    "        \"\"\"Returns True if the simulator is in a healthy state.\"\"\"\n",
    "        min_velocity, max_velocity = self.healthy_velocity_range\n",
    "        velocity = self.sim.data.qvel.flat[:]\n",
    "        healthy_velocity = np.all(np.logical_and(min_velocity < velocity, velocity < max_velocity))\n",
    "\n",
    "        healthy = healthy_velocity\n",
    "        return healthy\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"Resets the environment.\"\"\"\n",
    "        self.t = 0\n",
    "        return super().reset()\n",
    "\n",
    "    def step(self, *args, **kwargs) -> Tuple:\n",
    "        \"\"\"Fixes a non-json-writable element in the info of the base env.\"\"\"\n",
    "        obs, reward, done, info = super().step(*args, **kwargs)\n",
    "        info[\"reward_ctrl\"] = float(info[\"reward_ctrl\"])\n",
    "\n",
    "        if self.terminate_when_unhealthy and not self.is_healthy():\n",
    "            done = True\n",
    "\n",
    "        self.t += 1\n",
    "        if self.t >= self.max_timesteps:\n",
    "            done = True\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def _get_obs(self) -> np.ndarray:\n",
    "        \"\"\"Optionally overwrite the observation to for simulation purposes.\"\"\"\n",
    "        if self.obs_mode == \"sim\":\n",
    "            return np.concatenate([\n",
    "                self.sim.data.qpos.flat[:],\n",
    "                self.sim.data.qvel.flat[:],\n",
    "                self.get_body_com(\"fingertip\") - self.get_body_com(\"target\"),\n",
    "            ])\n",
    "        elif self.obs_mode == \"original\":\n",
    "            return super()._get_obs()\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid observation mode: {self.obs_mode}\")\n",
    "\n",
    "\n",
    "class CustomReacherEnvRewardModel(RewardModel):\n",
    "    \"\"\"Reward model for custom Reacher environment.\n",
    "\n",
    "    Args:\n",
    "        obs_space: The observation space used in the environment.\n",
    "        act_space: The action space used in the environment.\n",
    "        reward_dist_factor: Weight on the distance from goal reward term.\n",
    "        reward_ctrl_factor: Weight on the control reward term.\n",
    "        reward_goal_factor: Weight on reaching the goal.\n",
    "        shaping_factor: The value to scale the shaping.\n",
    "        shaping_discount: The discount factor used in potential shaping.\n",
    "    \"\"\"\n",
    "    # At this threshold around 2% of initial states are next to the goal.\n",
    "    GOAL_REACHED_THRESHOLD = 0.05\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            obs_space: gym.spaces.Space,\n",
    "            act_space: gym.spaces.Space,\n",
    "            reward_dist_factor: float,\n",
    "            reward_ctrl_factor: float,\n",
    "            reward_goal_factor: float,\n",
    "            shaping_factor: float,\n",
    "            shaping_discount: float,\n",
    "    ):\n",
    "        self.obs_space = obs_space\n",
    "        self.act_space = act_space\n",
    "        self.reward_dist_factor = reward_dist_factor\n",
    "        self.reward_ctrl_factor = reward_ctrl_factor\n",
    "        self.reward_goal_factor = reward_goal_factor\n",
    "        self.shaping_factor = shaping_factor\n",
    "        self.shaping_discount = shaping_discount\n",
    "\n",
    "    @property\n",
    "    def observation_space(self) -> gym.spaces.Space:\n",
    "        return self.obs_space\n",
    "\n",
    "    @property\n",
    "    def action_space(self) -> gym.spaces.Space:\n",
    "        return self.act_space\n",
    "\n",
    "    def reward(\n",
    "            self,\n",
    "            states: torch.Tensor,\n",
    "            actions: torch.Tensor,\n",
    "            next_states: Optional[torch.Tensor],\n",
    "            terminals: Optional[torch.Tensor],\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Computes the reward for the environment.\n",
    "\n",
    "        See base class for documentation on args and return value.\n",
    "        \"\"\"\n",
    "        del terminals\n",
    "        states_dists = states[:, -3:].norm(dim=-1, keepdim=True)\n",
    "        dist_rewards = -states_dists\n",
    "        ctrl_rewards = -actions.square().sum(dim=1, keepdim=True).to(states.dtype)\n",
    "        goal_rewards = states_dists < self.GOAL_REACHED_THRESHOLD\n",
    "\n",
    "        next_states_dists = next_states[:, -3:].norm(dim=-1, keepdim=True)\n",
    "        shaping_rewards = (self.shaping_discount * next_states_dists - states_dists)\n",
    "\n",
    "        rewards = self.reward_dist_factor * dist_rewards \\\n",
    "            + self.reward_ctrl_factor * ctrl_rewards \\\n",
    "            + self.reward_goal_factor * goal_rewards \\\n",
    "            + self.shaping_factor * shaping_rewards\n",
    "\n",
    "        return -rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions: [array([0.5103982 , 0.05854821], dtype=float32), array([0.05495619, 0.88524526], dtype=float32), array([0.54897666, 0.33826023], dtype=float32), array([0.7161359 , 0.88141185], dtype=float32), array([ 0.36318964, -0.21498232], dtype=float32), array([-0.78796184,  0.45962584], dtype=float32), array([-0.81977874,  0.7646321 ], dtype=float32), array([0.7595484, 0.5301372], dtype=float32), array([ 0.15576857, -0.03695793], dtype=float32), array([ 0.8190548, -0.685813 ], dtype=float32), array([ 0.39426157, -0.32317394], dtype=float32), array([-0.43361637, -0.67343277], dtype=float32), array([-0.66567177,  0.59845173], dtype=float32), array([0.20157081, 0.8550486 ], dtype=float32), array([ 0.5027176, -0.9290245], dtype=float32), array([0.46628687, 0.40892228], dtype=float32), array([0.935982  , 0.46178487], dtype=float32), array([-0.08829168,  0.63290375], dtype=float32), array([ 0.771909 , -0.5501507], dtype=float32), array([-0.78652126,  0.26874608], dtype=float32), array([-0.9001675 ,  0.62044317], dtype=float32), array([-0.28213292,  0.07260442], dtype=float32), array([-0.63727933,  0.553486  ], dtype=float32), array([ 0.7446731 , -0.86221933], dtype=float32), array([-0.63039887,  0.4557857 ], dtype=float32), array([ 0.1082579 , -0.30204177], dtype=float32), array([-0.15204312,  0.65035987], dtype=float32), array([-0.20411947, -0.5337695 ], dtype=float32), array([-0.6719628, -0.3677466], dtype=float32), array([0.41595358, 0.776691  ], dtype=float32), array([0.40077743, 0.79184276], dtype=float32), array([ 0.9713182, -0.475815 ], dtype=float32), array([0.8883105 , 0.12239534], dtype=float32), array([ 0.8240852, -0.6794024], dtype=float32), array([-0.04139753, -0.1778776 ], dtype=float32), array([-0.18988678, -0.83021325], dtype=float32), array([0.49342915, 0.05586752], dtype=float32), array([-0.7342388,  0.4598701], dtype=float32), array([ 0.30344903, -0.89788234], dtype=float32), array([-0.20902188,  0.92400336], dtype=float32), array([-0.51316833,  0.27233678], dtype=float32), array([-0.8452023 , -0.83223724], dtype=float32), array([-0.90833914, -0.11037625], dtype=float32), array([-0.34702232,  0.03517888], dtype=float32), array([ 0.72197413, -0.9973819 ], dtype=float32), array([-0.58839077,  0.7947033 ], dtype=float32), array([-0.48195934, -0.92377245], dtype=float32), array([-0.8268828 ,  0.64399076], dtype=float32), array([0.20975219, 0.9678305 ], dtype=float32), array([ 0.05875928, -0.77503765], dtype=float32), array([-0.7552768,  0.684162 ], dtype=float32), array([ 0.81393456, -0.72564787], dtype=float32), array([0.874027  , 0.46402648], dtype=float32), array([-0.7386491, -0.2702667], dtype=float32), array([ 0.660351 , -0.4879551], dtype=float32), array([-0.3055054,  0.6606389], dtype=float32), array([ 0.344917  , -0.34135816], dtype=float32), array([-0.6277544 , -0.04878942], dtype=float32), array([-0.3148179 ,  0.02821233], dtype=float32), array([-0.7606549 , -0.50476336], dtype=float32), array([ 0.7821695 , -0.45845962], dtype=float32), array([ 0.06999149, -0.13328603], dtype=float32), array([-0.17124689,  0.47452736], dtype=float32), array([-0.5037143 , -0.03930172], dtype=float32), array([-0.3030413,  0.5877226], dtype=float32), array([ 0.42998612, -0.24134961], dtype=float32), array([-0.6845864,  0.4307123], dtype=float32), array([-0.5311455 , -0.13465695], dtype=float32), array([-0.5163584 ,  0.42595696], dtype=float32), array([0.27425247, 0.6014769 ], dtype=float32), array([ 0.10443151, -0.1301991 ], dtype=float32), array([-0.14091012, -0.53835154], dtype=float32), array([ 0.35101196, -0.5582796 ], dtype=float32), array([ 0.66972375, -0.01652794], dtype=float32), array([-0.22795075,  0.12544174], dtype=float32), array([ 0.32958832, -0.986496  ], dtype=float32), array([0.43838167, 0.9055945 ], dtype=float32), array([-0.8501198,  0.1841996], dtype=float32), array([0.40804854, 0.98644656], dtype=float32), array([0.25091428, 0.18677026], dtype=float32), array([ 0.1751418, -0.918653 ], dtype=float32), array([-0.5129283 , -0.19204514], dtype=float32), array([ 0.67717236, -0.70361406], dtype=float32), array([0.003098 , 0.7938693], dtype=float32), array([ 0.575334  , -0.26174468], dtype=float32), array([ 0.43358466, -0.98674214], dtype=float32), array([-0.13860899,  0.7074877 ], dtype=float32), array([-0.89148307,  0.03205995], dtype=float32), array([-0.09533013, -0.11293188], dtype=float32), array([0.95809036, 0.58386534], dtype=float32), array([ 0.6481593, -0.6604774], dtype=float32), array([-0.06865812,  0.4372052 ], dtype=float32), array([ 0.87987137, -0.31793645], dtype=float32), array([ 0.27451703, -0.15478069], dtype=float32), array([0.20785107, 0.5710679 ], dtype=float32), array([-0.75228614, -0.17482778], dtype=float32), array([ 0.6792139 , -0.85664153], dtype=float32), array([-0.29403102, -0.6471072 ], dtype=float32), array([ 0.49568036, -0.3463218 ], dtype=float32), array([-0.9270515, -0.7512301], dtype=float32)]\n",
      "Rewards: [[-0.4828720211648293, -1.0118301439895954, -0.6559754360464946, -1.5343000448189794, -0.3689926907387213, -0.8890135928300109, -1.3099647346218184, -0.9038109413784837, -0.07325077671068264, -1.1967463429730973], [-0.49477437060717544, -0.8722800219706199, -1.0327499422826916, -1.0140867736656922, -1.3720285029259993, -0.6452197497920453, -1.3431356064877615, -0.6400080134420403, -1.0791090035529345, -0.8258033496242969], [-1.5400177048867367, -0.435465510093834, -1.066443440716847, -1.6318329239425302, -0.9006012171036482, -0.3509876704299734, -0.6276484600577381, -0.4485363522261247, -0.6808421354794614, -0.875430781474778], [-1.048201475858082, -1.4531230884830006, -1.1352590159752576, -1.4960117541168303, -0.31031986281432744, -0.8266088211044443, -0.35012527344432265, -0.9926306048612752, -1.218130253314798, -1.212784006115692], [-0.6651108567889944, -1.7240866599046358, -1.0866459487135147, -0.19043753790562018, -1.6914054118314317, -1.2994790355095467, -1.407177957640533, -1.2971457378508897, -1.1429783577098187, -0.803249912801672], [-1.3215782626324173, -1.4742630272923458, -1.2646019904890324, -0.8917924068932994, -0.9265674513878017, -0.770394195933075, -0.45366438312763746, -0.5849430173513213, -0.27196395608408686, -1.0093901330261532], [-0.9113186943718329, -0.1368153046713993, -0.41459784191693083, -0.4593109259083453, -0.6635859879664635, -0.4722492084261869, -0.8982447166523528, -0.5513625338033836, -0.6630229932976192, -0.5743037680038696], [-0.3873978819892555, -0.6686767342153647, -0.7976774533852605, -0.8092242275731, -0.40119018129306233, -1.3754628515870342, -1.2467439368769728, -0.9367470140045898, -1.3211791385879919, -0.26795948499860417], [-1.1650797679833813, -0.5998465993860027, -1.2790859460958668, -0.9556937361790878, -0.6905234950764862, -1.4125532085444261, -0.7175930408227813, -0.9901883979321252, -0.20279802364035263, -1.4255937607175089], [-1.060677024221921, -0.3891215304038689, -1.039573433869839, -0.20256985766648428, -0.41300758916019237, -0.7326042550509084, -1.4146051763488465, -0.7589970459219786, -0.6013573970066574, -1.5960763675145504]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "env = CustomReacherEnv()\n",
    "\n",
    "num_episodes = 10\n",
    "\n",
    "num_steps = 10\n",
    "\n",
    "observations = []\n",
    "actions = []\n",
    "rewards = []\n",
    "infos = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observation = env.reset()\n",
    "    observations.append([observation]) \n",
    "    episode_rewards = []\n",
    "    for _ in range(num_steps):\n",
    "        # Random action using the action space of CustomReacherEnv\n",
    "        action = env.action_space.sample()\n",
    "        actions.append(action)\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observations[-1].append(observation)  # Append new observation to the current episode\n",
    "        episode_rewards.append(reward)\n",
    "        infos.append(info)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(episode_rewards)  # Append episode rewards\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Now, you can print or analyze your values\n",
    "print(\"Actions:\", actions)\n",
    "print(\"Rewards:\", rewards)\n",
    "# And so on for other values...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1827 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f7c404ecf50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import PPO, MlpPolicy\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Wrap your custom environment. VecEnvs are typically used for better performance.\n",
    "env = DummyVecEnv([lambda: CustomReacherEnv()])\n",
    "\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO(MlpPolicy, env, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = [-109.59885]\n",
      "Episode 2: Total Reward = [-47.329876]\n",
      "Episode 3: Total Reward = [-115.77665]\n",
      "Episode 4: Total Reward = [-43.521034]\n",
      "Episode 5: Total Reward = [-117.84858]\n",
      "Episode 6: Total Reward = [-117.76264]\n",
      "Episode 7: Total Reward = [-40.20004]\n",
      "Episode 8: Total Reward = [-42.024567]\n",
      "Episode 9: Total Reward = [-44.834965]\n",
      "Episode 10: Total Reward = [-53.375034]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of episodes\n",
    "num_episodes = 10\n",
    "\n",
    "# Number of steps per episode\n",
    "num_steps = 200\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    for _ in range(num_steps):\n",
    "        # Predict the action using the trained policy\n",
    "        action, _ = model.predict(obs)\n",
    "        \n",
    "        # Step through the environment\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Accumulate the reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    print(f\"Episode {episode + 1}: Total Reward = {total_reward}\")\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0685595  -0.10764125  0.14699024  0.03250305 -2.28044644 -8.21489907\n",
      "  0.          0.          0.06107204 -0.05863334  0.        ]\n"
     ]
    }
   ],
   "source": [
    "def predict_next_state(inner_env, state, action):\n",
    "    # Set environment to desired state\n",
    "    inner_env.set_state(state[:inner_env.model.nq], state[inner_env.model.nq:inner_env.model.nq + inner_env.model.nv])\n",
    "    \n",
    "    # Take the desired action\n",
    "    next_state, _, _, _ = inner_env.step(action)\n",
    "    \n",
    "    return next_state\n",
    "\n",
    "# Extract the inner environment from DummyVecEnv\n",
    "inner_env = env.envs[0]\n",
    "\n",
    "# Example usage:\n",
    "current_state = env.reset()\n",
    "action, _ = model.predict(current_state)\n",
    "\n",
    "predicted_next_state = predict_next_state(inner_env, current_state[0], action)\n",
    "print(predicted_next_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state: [ 0.05441318 -0.08388495 -0.16901683  0.07022422  0.0013621  -0.00461935\n",
      "  0.          0.          0.37882106 -0.06802701  0.        ]\n",
      "Predicted next state: [ 0.20146486  0.16167626 -0.16901683  0.07022422  5.83224858  9.74584388\n",
      "  0.          0.          0.3698219  -0.01114397  0.        ]\n"
     ]
    }
   ],
   "source": [
    "def predict_next_state(inner_env, state, action):\n",
    "    inner_env.set_state(state[:inner_env.model.nq], state[inner_env.model.nq:inner_env.model.nq + inner_env.model.nv])\n",
    "    \n",
    "\n",
    "    next_state, _, _, _ = inner_env.step(action)\n",
    "    \n",
    "    return next_state\n",
    "\n",
    "inner_env = env.envs[0]\n",
    "\n",
    "current_state = env.reset()\n",
    "print(f\"Current state: {current_state[0]}\")\n",
    "\n",
    "action, _ = model.predict(current_state)\n",
    "\n",
    "predicted_next_state = predict_next_state(inner_env, current_state[0], action)\n",
    "print(f\"Predicted next state: {predicted_next_state}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: Max Value = 44071.9616018181\n",
      "Index 1: Max Value = 3.4224790841512687\n",
      "Index 2: Max Value = 0.06941229678717048\n",
      "Index 3: Max Value = 0.060335965865774166\n",
      "Index 4: Max Value = 95.70480091231406\n",
      "Index 5: Max Value = 43.25942034596264\n",
      "Index 6: Max Value = 0.0\n",
      "Index 7: Max Value = 0.0\n",
      "Index 8: Max Value = 0.14058531811141112\n",
      "Index 9: Max Value = 0.1496591108899175\n",
      "Index 10: Max Value = 0.0\n"
     ]
    }
   ],
   "source": [
    "def predict_next_state(inner_env, state, action):\n",
    "    inner_env.set_state(state[:inner_env.model.nq], state[inner_env.model.nq:inner_env.model.nq + inner_env.model.nv])\n",
    "    next_state, _, _, _ = inner_env.step(action)\n",
    "    return next_state\n",
    "\n",
    "inner_env = env.envs[0]\n",
    "\n",
    "# Initialize current state\n",
    "current_state = env.reset()\n",
    "\n",
    "\n",
    "max_values = [-float('inf') for _ in current_state[0]]\n",
    "\n",
    "for _ in range(100000):\n",
    "    action, _ = model.predict(current_state)\n",
    "    predicted_next_state = predict_next_state(inner_env, current_state[0], action)\n",
    "\n",
    "    max_values = [max(max_val, state_val) for max_val, state_val in zip(max_values, predicted_next_state)]\n",
    "    current_state[0] = predicted_next_state\n",
    "\n",
    "\n",
    "for idx, max_val in enumerate(max_values):\n",
    "    print(f\"Index {idx}: Max Value = {max_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 0: Max Value = 384.4965632065514\n",
      "Index 1: Max Value = 3.360668491593963\n",
      "Index 2: Max Value = -0.12060491121944264\n",
      "Index 3: Max Value = 0.10409613674874768\n",
      "Index 4: Max Value = 103.2610126885073\n",
      "Index 5: Max Value = 43.3217765100737\n",
      "Index 6: Max Value = 0.0\n",
      "Index 7: Max Value = 0.0\n",
      "Index 8: Max Value = 0.330601465127105\n",
      "Index 9: Max Value = 0.10589626741663938\n",
      "Index 10: Max Value = 0.0\n"
     ]
    }
   ],
   "source": [
    "def predict_next_state(inner_env, state, action):\n",
    "    inner_env.set_state(state[:inner_env.model.nq], state[inner_env.model.nq:inner_env.model.nq + inner_env.model.nv])\n",
    "    next_state, _, _, _ = inner_env.step(action)\n",
    "    return next_state\n",
    "\n",
    "inner_env = env.envs[0]\n",
    "\n",
    "# Initialize current state\n",
    "current_state = env.reset()\n",
    "\n",
    "\n",
    "max_values = [-float('inf') for _ in current_state[0]]\n",
    "\n",
    "for _ in range(100000):\n",
    "    action, _ = model.predict(current_state)\n",
    "    predicted_next_state = predict_next_state(inner_env, current_state[0], action)\n",
    "\n",
    "    max_values = [max(max_val, state_val) for max_val, state_val in zip(max_values, predicted_next_state)]\n",
    "    current_state[0] = predicted_next_state\n",
    "\n",
    "\n",
    "for idx, max_val in enumerate(max_values):\n",
    "    print(f\"Index {idx}: Max Value = {max_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03747448, -0.04509116,  0.05504468, -0.17726689,  0.00325098,\n",
       "         0.00069581,  0.        ,  0.        ,  0.15488192,  0.18017563,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of the given state: -3.272695779800415\n"
     ]
    }
   ],
   "source": [
    "#Values for states\n",
    "obs = [0.08370445, -0.28718871, -0.07374638, -0.15162033, 7.01024685, -8.4554024, 0., 0., 0.28112693, 0.13775099, 0.]\n",
    "obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(model.device)\n",
    "\n",
    "# Predict the value\n",
    "value_tensor = model.policy.predict_values(obs_tensor)\n",
    "\n",
    "# Convert the tensor to a Python number\n",
    "value = value_tensor.item()\n",
    "print(\"Value of the given state:\", value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -6.016434462293983 +/- 0.57\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "print(f\"Mean reward: {mean_reward} +/- {std_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3479661/2651618105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'obs' is not defined"
     ]
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "044c9d53ccecfacc6c4ac68be7cb18c801110633783336a1c040b95f9afc15b1"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cs285')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
