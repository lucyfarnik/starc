{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is something I'm currently messing around with, the point is to\n",
    "find a good analytical solution for DARD that's reasonably fast.\n",
    "\n",
    "**Scroll down to the my_dard function for the current implementation**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from distance import canon\n",
    "from _types import Reward\n",
    "from env import Env, RandomEnv\n",
    "from tests.toy_env import env, reward\n",
    "from coverage_dist import get_state_dist, get_action_dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual implementation where we calculate the EV terms as explicit sums via for loops\n",
    "def dard_slow(reward: Reward, env: Env):\n",
    "  action_dist = get_action_dist(env)\n",
    "\n",
    "  exp_term1 = np.zeros(env.n_s)\n",
    "  for s_prime in range(env.n_s):\n",
    "    for A, A_prob in enumerate(action_dist):\n",
    "      for S_double, S_double_prob in enumerate(env.transition_dist[s_prime, A]):\n",
    "        prob = A_prob * S_double_prob\n",
    "        exp_term1[s_prime] += prob * env.discount * reward[s_prime, A, S_double]\n",
    "  exp_term1 = exp_term1[None, None, :]\n",
    "\n",
    "  exp_term2 = np.zeros(env.n_s)\n",
    "  for s in range(env.n_s):\n",
    "    for A, A_prob in enumerate(action_dist):\n",
    "      for S_prime, S_prime_prob in enumerate(env.transition_dist[s, A]):\n",
    "        prob = A_prob * S_prime_prob\n",
    "        exp_term2[s] += prob * reward[s, A, S_prime]\n",
    "  exp_term2 = exp_term2[:, None, None]\n",
    "\n",
    "  exp_term3 = np.zeros((env.n_s, env.n_s))\n",
    "  for s in range(env.n_s):\n",
    "    for s_prime in range(env.n_s):\n",
    "      for A, A_prob in enumerate(action_dist):\n",
    "        for S_prime, S_prime_prob in enumerate(env.transition_dist[s, A]):\n",
    "          for S_double, S_double_prob in enumerate(env.transition_dist[s_prime, A]):\n",
    "            prob = A_prob * S_prime_prob * S_double_prob\n",
    "            exp_term3[s, s_prime] += prob * env.discount * reward[S_prime, A, S_double]\n",
    "  exp_term3 = exp_term3[:, None, :]\n",
    "\n",
    "  return exp_term1, exp_term2, exp_term3\n",
    "\n",
    "  return reward + exp_term1 - exp_term2 - exp_term3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(potential, gamma):\n",
    "  return (\n",
    "      gamma * potential[..., None, None, :] - potential[..., :, None, None]\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the file Joar sent over\n",
    "def dard_reference(rewards, weights, gamma):\n",
    "  \"\"\"Compute the DARD transformation of a given reward function.\"\"\"\n",
    "  weighted_rewards = rewards * weights\n",
    "  outflow = weighted_rewards.sum((-2, -1))\n",
    "  # outflow contains state distribution, which we want to remove:\n",
    "  potential = outflow / weights.sum(axis=(1, 2))\n",
    "  joint_state_probs = weights.sum(axis=1)\n",
    "  # next_state_probs[s, s'] = P(s' | s)\n",
    "  next_state_probs = joint_state_probs / joint_state_probs.sum(\n",
    "      axis=1, keepdims=True\n",
    "  )\n",
    "  action_probs = weights.sum(axis=(0, 2))\n",
    "  # I'm calling this an \"offset\" in analogy to EPIC, but note that it's not\n",
    "  # a constant!\n",
    "  offset = (\n",
    "      rewards[None, :, :, None, :]\n",
    "      * next_state_probs[:, :, None, None, None]\n",
    "      * action_probs[None, None, :, None, None]\n",
    "      * next_state_probs[None, None, None, :, :]\n",
    "  ).sum(axis=(1, 2, 4))\n",
    "\n",
    "  return gamma * potential[..., None, None, :], potential[..., :, None, None], gamma * offset[:, None, :]\n",
    "\n",
    "  return rewards + gradient(potential, gamma) - gamma * offset[:, None, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[[0.45, 0.  ]]]),\n",
       "  array([[[0.5]],\n",
       "  \n",
       "         [[0. ]]]),\n",
       "  array([[[0.  , 0.  ]],\n",
       "  \n",
       "         [[0.45, 0.  ]]])),\n",
       " (array([[[0.225, 0.   ]]]),\n",
       "  array([[[0.25]],\n",
       "  \n",
       "         [[0.  ]]]),\n",
       "  array([[[0.1125, 0.1125]],\n",
       "  \n",
       "         [[0.1125, 0.1125]]])))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let the fuckery commence!\n",
    "\n",
    "# 1 what the fuck are weights?\n",
    "slow_result = dard_slow(reward, env)\n",
    "D_s = get_state_dist(env)\n",
    "D_a = get_action_dist(env)\n",
    "weights = D_s[:, None, None] * D_a[None, :, None] * D_s[None, None, :]\n",
    "ref_result = dard_reference(reward, weights, env.discount)\n",
    "slow_result, ref_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[[0.1023023, 0.       , 0.       ]]]),\n",
       "  array([[[0.11366922]],\n",
       "  \n",
       "         [[0.        ]],\n",
       "  \n",
       "         [[0.        ]]]),\n",
       "  array([[[0.06387757, 0.12307836, 0.08056726]],\n",
       "  \n",
       "         [[0.03838355, 0.07395685, 0.04841226]],\n",
       "  \n",
       "         [[0.03305695, 0.06369364, 0.04169394]]])),\n",
       " (array([[[0.11059644, 0.        , 0.        ]]]),\n",
       "  array([[[0.12288494]],\n",
       "  \n",
       "         [[0.        ]],\n",
       "  \n",
       "         [[0.        ]]]),\n",
       "  array([[[0.04485786, 0.04485786, 0.04485786]],\n",
       "  \n",
       "         [[0.04485786, 0.04485786, 0.04485786]],\n",
       "  \n",
       "         [[0.04485786, 0.04485786, 0.04485786]]])))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env2 = RandomEnv(3, 2)\n",
    "reward2 = np.zeros((3, 2, 3))\n",
    "reward2[0, 1, 1] = 1.0\n",
    "\n",
    "D_s = get_state_dist(env2)\n",
    "D_a = get_action_dist(env2)\n",
    "weights = D_s[:, None, None] * D_a[None, :, None] * D_s[None, None, :]\n",
    "dard_slow(reward2, env2), dard_reference(reward2, weights, env.discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dard(reward: Reward, env: Env):\n",
    "  s = np.ones(env.n_s) # [s]\n",
    "  s_prime = np.ones(env.n_s) # [s_prime]\n",
    "  A = get_action_dist(env) # [A]\n",
    "\n",
    "  # Given a value of s, what are the probabilities of different A-S_prime combos\n",
    "  S_prime = env.transition_dist * s[:, None, None] * A[None, :, None] # [s, A, S_prime]\n",
    "\n",
    "  # Given a value of s_prime, what are the probabilities of different A-S_double combos\n",
    "  S_double = env.transition_dist * s_prime[:, None, None] * A[None, :, None] # [s_prime, A, S_double]\n",
    "\n",
    "  # gamma R(s_prime, A, S_double)\n",
    "  exp1 = env.discount * reward * S_double # [s, a, s_prime]\n",
    "  exp1 = exp1.sum(axis=(0, 1), keepdims=True) # [1, 1, s_prime]\n",
    "\n",
    "  # R(s, A, S_prime)\n",
    "  exp2 = reward * S_prime # [s, a, s_prime]\n",
    "  exp2 = exp2.sum(axis=(1, 2), keepdims=True) # [s, 1, 1]\n",
    "\n",
    "  # gamma R(S_prime, A, S_double)\n",
    "  exp3 = env.discount * reward * ( # [s, a, s_prime]\n",
    "    S_prime.sum(axis=(1, 2), keepdims=True) *\n",
    "    A[None, :, None] *\n",
    "    S_double.sum(axis=(0, 1), keepdims=True)\n",
    "  )\n",
    "  exp3 = exp3.sum(axis=1, keepdims=True) # [s, 1, s_prime]\n",
    "\n",
    "  return exp1, exp2, exp3\n",
    "\n",
    "  return reward + exp1 - exp2 - exp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.  , 0.45]]]),\n",
       " array([[[0.5]],\n",
       " \n",
       "        [[0. ]]]),\n",
       " array([[[0.  , 0.45]],\n",
       " \n",
       "        [[0.  , 0.  ]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# should output\n",
    "# ((array([[[0.45, 0.  ]]]),\n",
    "#   array([[[0.5]],\n",
    "#          [[0. ]]]),\n",
    "#   array([[[0.  , 0.  ]],\n",
    "#          [[0.45, 0.  ]]]))\n",
    "my_dard(reward, env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEA: sample expectation instead of the analytical solution if that doesn't work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "178f5955e7ec2db83c487531b6f19a6ba078d2c9bcad0eaf79872b0fcb34bd80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
