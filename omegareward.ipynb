{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from env import Env, RandomEnv\n",
    "from reward import random_reward\n",
    "from _types import Reward\n",
    "from utils import timed\n",
    "from canon import epic_canon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         d1        d2\n",
      "0  1.413613  1.426067\n",
      "1  1.413436  1.373728\n",
      "2  1.412767  1.401444\n",
      "3  1.414139  1.377690\n",
      "4  1.415693  1.405323\n",
      "5  1.413805  1.438628\n",
      "6  1.413378  1.335523\n",
      "7  1.415531  1.400022\n",
      "8  1.415986  1.429771\n",
      "9  1.415207  1.370721\n"
     ]
    }
   ],
   "source": [
    "#randomly generating 100 transition functions\n",
    "import numpy as np\n",
    "\n",
    "class Env():\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_s: int,\n",
    "        n_a: int,\n",
    "        discount: float,\n",
    "        init_dist: np.ndarray,\n",
    "        transition_dist: np.ndarray,\n",
    "    ):\n",
    "        self.n_s = n_s\n",
    "        self.states = np.arange(n_s)\n",
    "        self.n_a = n_a\n",
    "        self.actions = np.arange(n_a)\n",
    "        self.discount = discount\n",
    "        self.init_dist = init_dist\n",
    "        self.transition_dist = transition_dist\n",
    "\n",
    "class RandomEnv(Env):\n",
    "    def __init__(self, n_s: int = 128, n_a: int = 16, discount: int = 0.9):\n",
    "        init_dist = np.ones(n_s) / n_s\n",
    "        thresh = 1 if n_s < 50 else (1.5 if n_s < 100 else 1.8)\n",
    "        transition_dist = np.random.randn(n_s, n_a, n_s)\n",
    "        transition_dist = np.where(transition_dist > thresh,\n",
    "                             transition_dist, np.zeros_like(transition_dist) - 20)\n",
    "        transition_dist = softmax(transition_dist)\n",
    "        super().__init__(n_s, n_a, discount, init_dist, transition_dist)\n",
    "    def modified_reward_matrix(self, original_reward_matrix: np.ndarray) -> np.ndarray:\n",
    "        n_s, n_a, _ = original_reward_matrix.shape\n",
    "        modified_reward_matrix = np.zeros((n_s, n_a, n_s))\n",
    "\n",
    "        for s in range(n_s):\n",
    "            for a in range(n_a):\n",
    "                expected_reward = np.sum(self.transition_dist[s, a] * original_reward_matrix[s, a])\n",
    "                modified_reward_matrix[s, a] = expected_reward\n",
    "\n",
    "        return modified_reward_matrix\n",
    "\n",
    "def random_reward(env: Env) -> np.ndarray:\n",
    "    r = np.random.randn(env.n_s, env.n_a, env.n_s)\n",
    "    if np.random.random() > 0.8:\n",
    "        thresh = 3 if env.n_s < 50 else (3.5 if env.n_s < 100 else 3.8)\n",
    "        r = np.where(r > thresh, r, np.zeros_like(r))\n",
    "    if np.random.random() > 0.3:\n",
    "        r *= 10 * np.random.random()\n",
    "    if np.random.random() > 0.7:\n",
    "        r += 10 * np.random.random()\n",
    "    if np.random.random() > 0.5:\n",
    "        potential = np.random.randn(env.n_s)\n",
    "        potential *= 10 * np.random.random()\n",
    "        potential += np.random.random()\n",
    "        r += env.discount * potential[None, None, :] - potential[:, None, None]\n",
    "    return r\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "    #Cause why not\n",
    "\n",
    "def epic(r1: np.ndarray, r2: np.ndarray, env: Env) -> float:\n",
    "    r1_can = epic_canon(r1, env)\n",
    "    r2_can = epic_canon(r2, env)\n",
    "\n",
    "    r1_norm = r1_can / np.linalg.norm(r1_can.flatten(), 2)\n",
    "    r2_norm = r2_can / np.linalg.norm(r2_can.flatten(), 2)\n",
    "\n",
    "    return np.linalg.norm((r1_norm - r2_norm).flatten(), 2)\n",
    "\n",
    "def random_transition(env: Env) -> np.ndarray:\n",
    "    random_factor = np.random.uniform(0.8, 1.2)\n",
    "    thresh = random_factor * (1 if env.n_s < 50 else (1.5 if env.n_s < 100 else 1.8))\n",
    "    \n",
    "    mean = np.random.uniform(-1, 1)\n",
    "    std_dev = np.random.uniform(0.5, 2)\n",
    "    transition_dist = mean + std_dev * np.random.randn(env.n_s, env.n_a, env.n_s)\n",
    "    \n",
    "    sparsity = np.random.uniform(0.1, 0.9)\n",
    "    transition_dist = np.where(transition_dist > thresh, transition_dist, np.zeros_like(transition_dist) - 20)\n",
    "    \n",
    "    transition_dist = softmax(transition_dist)\n",
    "    \n",
    "    if np.random.random() > 0.5:\n",
    "        permute_states = np.random.permutation(env.n_s)\n",
    "        permute_actions = np.random.permutation(env.n_a)\n",
    "        transition_dist = transition_dist[permute_states, :, :]\n",
    "        transition_dist = transition_dist[:, permute_actions, :]\n",
    "    \n",
    "    return transition_dist\n",
    "\n",
    "import pandas as pd\n",
    "results = pd.DataFrame(columns=[\"d1\", \"d2\"])\n",
    "\n",
    "for i in range(10):\n",
    "    env = RandomEnv(n_s=128, n_a=16, discount=0.9)\n",
    "    r1 = random_reward(env)\n",
    "    r2 = random_reward(env)\n",
    "    r1_tau = env.modified_reward_matrix(r1)\n",
    "    r2_tau = env.modified_reward_matrix(r2)\n",
    "    transition_dist = random_transition(env)\n",
    "    env.transition_dist = transition_dist\n",
    "    d1 = epic(r1, r2, env)\n",
    "    num_transition_functions = 100\n",
    "    distances = []\n",
    "    for _ in range(num_transition_functions):\n",
    "        transition_dist = random_transition(env)\n",
    "        env.transition_dist = transition_dist\n",
    "\n",
    "        distance = epic(r1_tau, r2_tau, env)\n",
    "        distances.append(distance)\n",
    "\n",
    "    d2 = np.max(distances)\n",
    "    results.loc[i] = [d1, d2]\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genetic Algorithm to find max for the transition function\n",
    "def genetic_algorithm(\n",
    "    env: Env,\n",
    "    r1: np.ndarray,\n",
    "    r2: np.ndarray,\n",
    "    epic_function,\n",
    "    population_size: int = 100,\n",
    "    n_generations: int = 100,\n",
    "    mutation_rate: float = 0.1,\n",
    "    crossover_rate: float = 0.8,\n",
    "    selection_method: str = \"tournament\",\n",
    "    tournament_size: int = 5,\n",
    ") -> np.ndarray:\n",
    "\n",
    "    def crossover(parent1: np.ndarray, parent2: np.ndarray) -> np.ndarray:\n",
    "        n_s, n_a, _ = parent1.shape\n",
    "        crossover_point = np.random.randint(1, n_s * n_a)\n",
    "        \n",
    "        parent1_flat = parent1.reshape(-1)\n",
    "        parent2_flat = parent2.reshape(-1)\n",
    "        \n",
    "        offspring_flat = np.concatenate((parent1_flat[:crossover_point], parent2_flat[crossover_point:]))\n",
    "        offspring = offspring_flat.reshape((n_s, n_a, n_s))\n",
    "        \n",
    "        offspring = softmax(offspring)\n",
    "        return offspring\n",
    "\n",
    "    def mutate(individual: np.ndarray, mutation_strength: float = 0.05) -> np.ndarray:\n",
    "        mutation = (np.random.randn(*individual.shape) * mutation_strength)\n",
    "        mutated_individual = individual + mutation\n",
    "        \n",
    "        mutated_individual = softmax(mutated_individual)\n",
    "        return mutated_individual\n",
    "\n",
    "    def tournament_selection(population: List[np.ndarray], fitnesses: List[float]) -> List[np.ndarray]:\n",
    "        selected = []\n",
    "        for _ in range(tournament_size):\n",
    "            selected.append(random.choice(population))\n",
    "\n",
    "        selected_fitnesses = [fitnesses[next(i for i, p in enumerate(population) if np.array_equal(individual, p))] for individual in selected]\n",
    "        winner = selected[np.argmax(selected_fitnesses)]\n",
    "        return winner\n",
    "\n",
    "    def create_population() -> List[np.ndarray]:\n",
    "        return [random_transition(env) for _ in range(population_size)]\n",
    "\n",
    "    def evaluate_fitness(population: List[np.ndarray]) -> List[float]:\n",
    "        return [\n",
    "            epic_function(r1, r2, Env(n_s=env.n_s, n_a=env.n_a, discount=env.discount, init_dist=env.init_dist, transition_dist=individual))\n",
    "            for individual in population\n",
    "        ]\n",
    "\n",
    "    population = create_population()\n",
    "    best_individual = None\n",
    "    best_fitness = -np.inf\n",
    "\n",
    "    for generation in range(n_generations):\n",
    "        fitnesses = evaluate_fitness(population)\n",
    "\n",
    "        new_population = []\n",
    "        for _ in range(population_size):\n",
    "            if selection_method == \"tournament\":\n",
    "                parent1 = tournament_selection(population, fitnesses)\n",
    "                parent2 = tournament_selection(population, fitnesses)\n",
    "\n",
    "            if random.random() < crossover_rate:\n",
    "                offspring = crossover(parent1, parent2)\n",
    "            else:\n",
    "                offspring = random.choice([parent1, parent2])\n",
    "\n",
    "            if random.random() < mutation_rate:\n",
    "                offspring = mutate(offspring)\n",
    "\n",
    "            new_population.append(offspring)\n",
    "\n",
    "        population = new_population\n",
    "        best_in_generation = population[np.argmax(fitnesses)]\n",
    "        best_fitness_in_generation = max(fitnesses)\n",
    "\n",
    "        if best_fitness_in_generation > best_fitness:\n",
    "            best_fitness = best_fitness_in_generation\n",
    "            best_individual = best_in_generation\n",
    "\n",
    "    return best_individual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best transition matrix found:\n",
      "1.3916893546664557\n"
     ]
    }
   ],
   "source": [
    "def epic(r1: Reward, r2: Reward, env: Env) -> float:\n",
    "  r1_can = epic_canon(r1, env)\n",
    "  r2_can = epic_canon(r2, env)\n",
    "\n",
    "  r1_norm = r1_can / np.linalg.norm(r1_can.flatten(), 2)\n",
    "  r2_norm = r2_can / np.linalg.norm(r2_can.flatten(), 2)\n",
    "\n",
    "  return np.linalg.norm((r1_norm - r2_norm).flatten(), 2)\n",
    "\n",
    "env = RandomEnv(n_s=128, n_a=16, discount=0.9)\n",
    "\n",
    "\n",
    "r1 = random_reward(env)\n",
    "r2 = random_reward(env)\n",
    "r1_tau = env.modified_reward_matrix(r1)\n",
    "r2_tau = env.modified_reward_matrix(r2)\n",
    "\n",
    "\n",
    "best_transition = genetic_algorithm(\n",
    "    env=env,\n",
    "    r1=r1_tau,\n",
    "    r2=r2_tau,\n",
    "    epic_function=epic,  \n",
    "    population_size=100,\n",
    "    n_generations=100,\n",
    "    mutation_rate=0.1,\n",
    "    crossover_rate=0.8,\n",
    "    selection_method=\"tournament\",\n",
    "    tournament_size=5,\n",
    ")\n",
    "\n",
    "print(\"Best transition matrix found:\")\n",
    "\n",
    "env.transition_dist = best_transition\n",
    "distance = epic(r1_tau, r2_tau, env)\n",
    "print(distance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     d_epic     d_tau   d_omega\n",
      "0  1.413735  1.407244  1.434847\n",
      "1  1.414719  1.434938  1.412487\n",
      "2  1.415912  1.439695  1.434466\n",
      "3  1.411969  1.431157  1.403604\n",
      "4  1.414315  1.394433  1.397628\n",
      "5  1.415507  1.444671  1.403562\n",
      "6  1.413704  1.351617  1.363365\n",
      "7  1.411077  1.418128  1.417163\n",
      "8  1.413350  1.415324  1.402497\n",
      "9  1.414237  1.401149  1.389928\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "import random\n",
    "results = pd.DataFrame(columns=[\"d_epic\", \"d_tau\", \"d_omega\"])\n",
    "\n",
    "for i in range(10):\n",
    "    env = RandomEnv(n_s=128, n_a=16, discount=0.9)\n",
    "    r1 = random_reward(env)\n",
    "    r2 = random_reward(env)\n",
    "    r1_tau = env.modified_reward_matrix(r1)\n",
    "    r2_tau = env.modified_reward_matrix(r2)\n",
    "\n",
    "    #WHY??\n",
    "    #transition_dist = random_transition(env)\n",
    "    #env.transition_dist = transition_dist\n",
    "\n",
    "    d1 = epic(r1, r2, env)\n",
    "    # num_transition_functions = 1000\n",
    "    # distances = []\n",
    "    # for _ in range(num_transition_functions):\n",
    "    #     transition_dist = random_transition(env)\n",
    "    #     env.transition_dist = transition_dist\n",
    "\n",
    "    #     distance = epic(r1_tau, r2_tau, env)\n",
    "    #     distances.append(distance)\n",
    "    #d2 = np.max(distances)\n",
    "\n",
    "    d2 = epic(r1_tau, r2_tau, env)\n",
    "    x = genetic_algorithm(\n",
    "    env=env,\n",
    "    r1=r1_tau,\n",
    "    r2=r2_tau,\n",
    "    epic_function=epic,  \n",
    "    population_size=10,\n",
    "    n_generations=10,\n",
    "    mutation_rate=0.1,\n",
    "    crossover_rate=0.8,\n",
    "    selection_method=\"tournament\",\n",
    "    tournament_size=5,\n",
    "    )\n",
    "    env.transition_dist = x\n",
    "    \n",
    "    #Easy fix\n",
    "    #Need to generate r1_tau and r2_tau again using the new transition!!!\n",
    "    r1_tau_omega = env.modified_reward_matrix(r1)\n",
    "    r2_tau_omega = env.modified_reward_matrix(r2)\n",
    "\n",
    "    d3 = epic(r1_tau_omega, r2_tau_omega, env)\n",
    "    results.loc[i] = [d1, d2, d3]\n",
    "print(results)\n",
    "\n",
    "#d1 is epic(r1, r2, env) and d3 is epic(r1_tau, r2_tau, env) for d^\\omega(R1, R2) = max_{\\tau} d^\\tau(R1, R2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
  },
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
